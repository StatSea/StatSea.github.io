---
layout: single
title: "[DL] 딥러닝 05wk"
categories: [DL]
tags: [DL]
mathjax: true
---

딥러닝 05wk 이해하기

---

# 기본 세팅

```python
import torch
import matplotlib.pyplot as plt

plt.rcParams['figure.figsize'] = (4.5, 3.0)
```

# 1. 예측

- 예측하는 것이 우리의 목표임을 알아두자.

## A. 데이터
```python
torch.manual_seed(43052)
x,_ = torch.randn(100).sort()
eps = torch.randn(100)*0.5
y = x * 4 + 2.5 + eps
x,y = x.reshape(-1,1), y.reshape(-1,1)

plt.plot(x,y,'o')
```

![image](https://github.com/user-attachments/assets/be5d86e4-3798-49ce-bcb1-4fe0617a93e7)

## B. 학습

```python
net = torch.nn.Sequential(
    torch.nn.Linear(1,1)
)
loss_fn = torch.nn.MSELoss()
optimizr = torch.optim.Adam(net.parameters(),lr=0.1)
## 
for epoc in range(200):
    ## step1 
    yhat = net(x) 
    ## step2 
    loss = loss_fn(yhat,y)
    ## step3 
    loss.backward()
    ## step4 
    optimizr.step()
    optimizr.zero_grad()

plt.plot(x,y,'o')
plt.plot(x,net(x).data,'--')
```
![image](https://github.com/user-attachments/assets/cc53e41c-6ebb-4333-a968-e485ae164a86)

- 예측값을 확인해보기 ( 기울기와 절편 )
```python
net[0].weight, net[0].bias
```


## C. 예측

- 온도가 0.1도 일때, 커피를 얼마나 팔까?

```python
# 0.1 * 4.0042 + 2.4459

xx = torch.tensor([[0.1]])
net(xx)
```

- x값에 0.1을 넣은 효과와 같다. = 예측값을 구한 것

- 온도가 0.2도 일때, 커피를 얼마나 팔까?
  
```python
# 0.2 * 4.0042 + 2.4459

xx = torch.tensor([[0.2]])
net(xx)
```

- 온도가 [0.1, 0.2] 일때의 예측값을 한번에 보고 싶다면?

```python
xx = torch.tensor([[0.1],
                   [0.2]])
net(xx)
```

- 출력 : tensor([[2.8463], [3.2467]], grad_fn=<AddmmBackward0>)



# 2. 오버피팅 (시벤코정리의 이면)

## A. 오버피팅

- 오버피팅 : 데이터 = 언더라잉 + 오차 라고 생각할 때 우리가 데이터로부터 적합할 것은 언더라잉인데 오차항을 적합하고 있는 현상

## B. 오버피팅 예시

- 맞추지 말아야 할 것들도 맞추는 경우

![image](https://github.com/user-attachments/assets/37d4fb88-7d76-4806-b163-b7ad9b13c068)

```python
torch.manual_seed(5) 
x = torch.linspace(0,1,100).reshape(100,1)
y = torch.randn(100).reshape(100,1)*0.01
plt.plot(x,y,'--o',alpha=0.5)
```

![image](https://github.com/user-attachments/assets/11d22459-dbd1-4c23-aee1-fe46c9d9182f)

```python
torch.manual_seed(1)
net = torch.nn.Sequential(
    torch.nn.Linear(1,512),
    torch.nn.ReLU(),
    torch.nn.Linear(512,1)
)
loss_fn = torch.nn.MSELoss()
optimizr = torch.optim.Adam(net.parameters())
#---#
for epoc in range(1000):
    ## step1 
    yhat = net(x) 
    ## step2 
    loss = loss_fn(yhat,y)
    ## step3 
    loss.backward()
    ## step4 
    optimizr.step()
    optimizr.zero_grad()

plt.plot(x,y,'--o',alpha=0.5)
plt.plot(x,net(x).data,'--')
```

![image](https://github.com/user-attachments/assets/610a53dd-6fe2-4fe0-a627-206d1f6f760f)


## C. 오버피팅이라는 뚜렷한 증거! (train/test)

```python
torch.manual_seed(5) 
x_all = torch.linspace(0,1,100).reshape(100,1)
y_all = torch.randn(100).reshape(100,1)*0.01
x,xx = x_all[:80], x_all[80:]
y,yy = y_all[:80], y_all[80:]
plt.plot(x,y,'--o',alpha=0.5,label="training")
plt.plot(xx,yy,'--o',alpha=0.5,label="test")
plt.legend()
```
![image](https://github.com/user-attachments/assets/87cd3a79-479d-438d-beef-2609fb01f48b)


- train 만 학습

```python
torch.manual_seed(1)
net = torch.nn.Sequential(
    torch.nn.Linear(1,512),
    torch.nn.ReLU(),
    torch.nn.Linear(512,1)
)
loss_fn = torch.nn.MSELoss()
optimizr = torch.optim.Adam(net.parameters())
#---#
for epoc in range(1000):
    ## step1 
    yhat = net(x) 
    ## step2 
    loss = loss_fn(yhat,y)
    ## step3 
    loss.backward()
    ## step4 
    optimizr.step()
    optimizr.zero_grad()
```

- training data로 학습한 net을 training data 에 적용

```python
plt.plot(x_all,y_all,'--o',alpha=0.5,color="gray")
plt.plot(x,net(x).data,'--')
```
![image](https://github.com/user-attachments/assets/f9fde781-65ef-41b6-b493-18ce780b2be9)

- training에서는 그럭저럭 잘 맞춘다.
- training data로 학습한 net을 test data에 적용

```python
plt.plot(x_all,y_all,'--o',alpha=0.5,color="gray")
plt.plot(x,net(x).data,'--')
plt.plot(xx,net(xx).data,'--')
```

![image](https://github.com/user-attachments/assets/cde19845-973f-4407-9659-8fcb290bf08b)

- test에서 엉망인 경우가 발생함.. => overfit


## D. 시벤코정리의 올바른 이해
![image](https://github.com/user-attachments/assets/89094083-3eb9-4810-abfc-4fee7f1c6306)

---

# 3. 드랍아웃

## A. 오버피팅의 해결

- 오버피팅의 해결책 : 드랍아웃

- 데이터

```python
torch.manual_seed(5) 
x_all = torch.linspace(0,1,100).reshape(100,1)
y_all = torch.randn(100).reshape(100,1)*0.01
#plt.plot(x_all,y_all,'--o',alpha=0.5)
x,y = x_all[:80], y_all[:80]
xx,yy = x_all[80:], y_all[80:]
plt.plot(x,y,'--o',color="C0")
plt.plot(xx,yy,'--o',color="C1")
```

![image](https://github.com/user-attachments/assets/e196c752-7818-4ac4-895d-5027fba76520)


- 학습

```python
torch.manual_seed(1)
net = torch.nn.Sequential(
    torch.nn.Linear(1,512),
    torch.nn.ReLU(),
    torch.nn.Dropout(0.8),
    torch.nn.Linear(512,1)
)
loss_fn = torch.nn.MSELoss()
optimizr = torch.optim.Adam(net.parameters())
#---#
for epoc in range(1000):
    ## step1 
    yhat = net(x) 
    ## step2 
    loss = loss_fn(yhat,y)
    ## step3 
    loss.backward()
    ## step4 
    optimizr.step()
    optimizr.zero_grad()
```

- 결과 시각화 (잘못된 사용)

```python
plt.plot(x_all,y_all,'--o',alpha=0.5,color="gray")
plt.plot(x,net(x).data,'--')
plt.plot(xx,net(xx).data,'--')
```
![image](https://github.com/user-attachments/assets/87d5d126-6f85-4220-abe9-527f75f27b2c)


- 결과 시각화 (올바른 사용)

```python
net.training # 학습모드인지 확인

net.eval() # 추론, 평가모드로 전환

net.training # 학습모드 인지 확인

plt.plot(x_all,y_all,'--o',alpha=0.5,color="gray")
plt.plot(x,net(x).data,'--')
plt.plot(xx,net(xx).data,'--')
```
![image](https://github.com/user-attachments/assets/b2265ac6-bb24-4f80-b346-b7b138ffce99)

![image](https://github.com/user-attachments/assets/9ed76c30-edd5-4695-9877-e8c657b8fdcf)



## B. 드랍아웃 레이어

`-` 드랍아웃의 성질 1 : 드랍아웃의 계산방식을 이해해보자

```python
u = torch.randn(10,2)
d = torch.nn.Dropout(0.9)
u
```
![image](https://github.com/user-attachments/assets/70d7e63d-771b-4bc9-b5a2-1a9b1992aca5)


```python
d(u)
```
![image](https://github.com/user-attachments/assets/4bbc4bb0-de66-496c-a828-e1c586c542cb)


- 90%의 드랍아웃 : 드랍아웃층의 입력 중 임의로 90%를 골라서 결과를 0으로 만든다. + 그리고 0이 되지 않고 살아남은 값들은 10배 만큼 값이 커진다.
- 10배 키우는 이유? : 출력의 평균값을 보정하기 위해서

`-` 드랍아웃의 성질 2 : 드랍아웃을 on/off하는 방법을 이해해보자.

```python
u = torch.randn(10,2)
u
```
![image](https://github.com/user-attachments/assets/587fdcd9-d7ad-4908-8f7b-4d4f049779a7)


```python
net = torch.nn.Sequential(
    torch.nn.Dropout(0.9)
)
net
```
![image](https://github.com/user-attachments/assets/e58d6d4f-8085-4a34-90bc-77096a57d1e1)

```python
u, net(u)
```
![image](https://github.com/user-attachments/assets/c88f7329-ced0-4a74-8e95-bf4ad00250b9)

```python
net.training
```

- true

```python
net.eval() # 드랍아웃이 무력화
```
![image](https://github.com/user-attachments/assets/8cc7b5da-ce4f-4981-8db9-13851ac74c03)


```python
u,net(u)
```
![image](https://github.com/user-attachments/assets/2fba725b-8002-470b-b2e1-6a3c7a0728a6)


`-` 드랍아웃 레이어 정리

- 계산 : 입력의 일부를 임의로 0으로 만드는 역할, 0이 안된것들은 스칼라배하여 드랍아웃을 통과한 모든 숫자들의 총합이 대체로 일정하게 되도록 조정
- on/off : 학습시에는 드랍아웃, 학습을 안할때는 드랍오프
- 느낌 : 일부러 패널티를 안고 학습하는 느낌
- 효과 : 오버피팅을 억제하는 효과가 있음
-  이 방법으로만 오버피팅을 잡는것은 아니다...

## C. 드랍아웃 레이어의 위치

- 렐루, 드랍아웃의 특이한 성질 : dropout(relu(x)) = relu(dropout(x))

```python
u = torch.randn(10,2)
r = torch.nn.ReLU()
d = torch.nn.Dropout()

torch.manual_seed(0)
d(r(u)) , r(d(u)) 
```
![image](https://github.com/user-attachments/assets/eb04fd9a-ac34-4454-95dd-ef8b0f67aa7a)

- 값이 서로 동일하다..
- 다른 활성화 함수들은 성립안하므로 주의하기

```python
u = torch.randn(10,2)
s = torch.nn.Sigmoid()
d = torch.nn.Dropout()

torch.manual_seed(0)
d(s(u)) , s(d(u))
```
![image](https://github.com/user-attachments/assets/a9a4d17b-e3b0-447f-a635-a1436a505efe)
![image](https://github.com/user-attachments/assets/d1029419-be32-40e1-ae40-426a5896adda)

- 결론 : 드랍아웃은 활성화 함수 바로 뒤에 오는게 맞음.. 그렇지 않다면 0이 만들어지지 않음
- relu는 상관없음





























