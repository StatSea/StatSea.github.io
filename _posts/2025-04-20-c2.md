---
layout: single
title: "[ML] 기계학습 Chap 3 코드 정리"
categories: [ML]
tags: [ML]
mathjax: true
---
기계학습 Chap 3 코드 정리

---

# 1. 기본 세팅
```python
import numpy as np
import pandas as pd
!pip install matplotlib
import matplotlib.pyplot as plt
from matplotlib.pyplot import subplots

# 통계관련한 패키지를 모아둔것
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor as VIF
from statsmodels.stats.anova import anova_lm

# 데이터도 있고 함수도 있고...
!pip install ISLP
# !pip install --upgrade pandas
from ISLP import load_data
```

# 2. 단순선형 회귀분석
- Boston data 불러오기

```python
from ISLP.models import (ModelSpec as MS, summarize , poly)
from pandas.io.formats.format import DataFrameFormatter
# Inside the pandas/io/formats/html.py file (you'll need to find this file in your pandas installation),
# locate the _get_columns_formatted_values function and modify it as follows:
def _get_columns_formatted_values(self) -> list[str]:
        # only reached with non-Multi Index
        # return self.columns._format_flat(include_name=False)  # Replace this line
  formatter = DataFrameFormatter(self.columns, include_name=False)  # With this line
  return formatter._format_col_name_split()

Boston = load_data("Boston")
Boston.columns
X = pd.DataFrame({'intercept': np.ones(Boston.shape[0]), 'lstat': Boston['lstat']})
# 절편을 포함시키는 데이터프레임 생성
print(X[:4])

# 중위수와 관련된 통계량
y = Boston['medv']

```
- Boston 데이터에는 위도와 경도 등 집값에 관한 데이터들이 들어있다.
- 변수의 개수 * 확률만큼 잘못된 변수가 포함된다.

```python
model = sm.OLS(y, X)
results = model.fit()
print(results.summary())
# medv ~ lstat
# (adj.) R square / DF (1/504) / Durbin-Watson (auto-cor.)
```

![image](https://github.com/user-attachments/assets/268c6027-d626-4053-bcee-ab7b675c7ae5)

- 모형 : medv = 34.5538 - 0.9500 × lstat
- lstat이 1단위 증가할 때 medv는 0.95단위 감소한다.
- 회귀 계수는 전부 유의미하다.
  
|항목|	값|	해석|
|---|---|---|
|R-squared	|0.544	|설명력: lstat가 medv의 약 54.4%를 설명|
|Adj. R-squared	|0.543	|변수 수 보정한 설명력, 거의 같음 ⇒ 모델 적절|
|F-statistic	|601.6	|모델 전체의 유의성 검정: 매우 큼 (좋음)|
|Prob (F-statistic)|	5.08e-88	|p값 ≈ 0 ⇒ 모델이 통계적으로 유의미함|
|AIC / BIC|	3287 / 3295|	다른 모델 비교 시 사용할 정보 기준값 (낮을수록 좋음)|
|Durbin-Watson|	0.892|	2에 가까울수록 좋음, 0.892는 양의 자기상관 있음 의심 가능|

- 정규성 진단

|항목	|값|	해석|
|---|---|---|
|Omnibus / Prob(Omnibus)|	137.04 / 0.000|	정규성 가정 위반 의심 (p < 0.05)|
|Jarque-Bera (JB)	|291.373|	JB 통계량 큼 → 정규성 위반 강하게 의심|
|Skew / Kurtosis	|1.45 / 5.32|	왜도 있음 / 첨도 높음 ⇒ 정규분포보다 뾰족하고 치우침|

- 왜도 : 기준 0
- 첨도 : 기준 3

|JB 값|	p-value	해석|
|---|---|
|작음	|> 0.05	정규성 만족 (귀무가설 기각 못 함)|
|큼	|< 0.05	정규성 위배 (귀무가설 기각)|

```python
# MS 구문을 이용해서 입력 행렬 처리
#--------------------------
design = MS(['lstat'])
design = design.fit(Boston)
X = design.transform(Boston)
print(X[:4])
model = sm.OLS(y, X)
results = model.fit()
print(results.params)
```

```python
# 새로운 입력변수에 대한 예측 (평균, 신뢰구간, 예측구간)
# -----------------------------------------
new_df = pd.DataFrame({'lstat':[5, 10, 15]})
newX = design.transform(new_df)
print(newX)
new_predictions = results.get_prediction(newX);
print(new_predictions.predicted_mean) # 평균
print(new_predictions.conf_int(alpha=0.05))
print(new_predictions.conf_int(obs=True, alpha=0.05))

# 집값이 어떠냐고 묻는다면 19. 머시기 사용
```

```python
# 평균직선에 대한 그래프 / 잔차에 대한 그래프
# ---------------------------------

import matplotlib.pyplot as plt

def abline(ax, b, m):
  xlim = ax.get_xlim()
  ylim = [m * xlim[0] + b, m * xlim[1] + b]
  ax.plot(xlim, ylim)

def abline(ax, b, m, *args, **kwargs):
  xlim = ax.get_xlim()
  ylim = [m * xlim[0] + b, m * xlim[1] + b]
  ax.plot(xlim, ylim, *args, **kwargs)

ax = subplots(figsize=(5,5))[1]
abline(ax,
results.params[0],
results.params[1], 'r--', linewidth=3)

ax = subplots(figsize=(8,8))[1]
ax.scatter(results.fittedvalues, results.resid)
ax.set_xlabel('Fitted value')
ax.set_ylabel('Residual')
ax.axhline(0, c='k', ls='--');
```

- 결과
![image](https://github.com/user-attachments/assets/6a8b7810-c6be-478a-ba96-318fb7ec2d3f)
![image](https://github.com/user-attachments/assets/562f063b-b1fc-4cf8-bf5d-e4c532506046)

- 잔차의 평균 함수 그림이 선형적이지 않고 곡선형태를 띔

```python
# 관측치에 대한 leverage 계산 및 그래프화
# --------------------------------
infl = results.get_influence()
ax = subplots(figsize=(8,8))[1]

# 레버리지 = 어떠한 관측치는 영향이 크고 어떠한 관측치는 영향이 작은것을 나타냄
# 튀어나온 데이터들은 뭔지 한번 보는 정도
ax.scatter(np.arange(X.shape[0]), infl.hat_matrix_diag)
ax.set_xlabel('Index')
ax.set_ylabel('Leverage')
np.argmax(infl.hat_matrix_diag)
```
![image](https://github.com/user-attachments/assets/ae8cf802-6525-404a-a658-f47ccf823b45)

- 레버리지가 특이하게 높은 점들이 있음

## Boston data 다중선형 회귀 분석
```python
# 입력변수를 lstat, age로 확장
# -----------------------

# 연령? 집의 연령?
X = MS(['lstat', 'age']).fit_transform(Boston)
model1 = sm.OLS(y, X)
results1 = model1.fit()
print(summarize(results1))
```

```python
# 입력변수를 mdev를 제외한 모든 변수로 학장
# --------------------------------
# p>t가 유의 확률
# 0,05보다 작냐 크냐를 봄
# 0.05보다 큰 애가 2개가 있음
# 0이 아니라는 것을 통과를 못했으므로 빼는게 나음
# 변수의 개수가 많을때는 실수할 수도 있으니까 이걸 보고 판정하는 것보다 다른 것을 보고 측정을 해야함
# AIC, BIC를 봄

terms = Boston.columns.drop('medv')
terms
X = MS(terms).fit_transform(Boston)
model = sm.OLS(y, X)
results = model.fit()
print(summarize(results))
```

```python
# 위의 변수 중 age를 제외한 결과 (앞의 표에서 age이 p.vlaue가 높음에 유의)
# result의 summary도 한번 봐야함 (나중에 해보세요?)

minus_age = Boston.columns.drop(['medv', 'age'])
Xma = MS(minus_age).fit_transform(Boston)
model1 = sm.OLS(y, Xma)
print(summarize(model1.fit()))
```
![image](https://github.com/user-attachments/assets/59b51979-77b8-4e4a-a678-5875de911a2e)

```python
# 다중공선성을 확인하기 위한 VIF 값을 보여줌
# --------------------------------
# 변수들이 서로 엉켜있는 것
# 밑에 있는 애들이 선형 결합으로 잘 표현된다는 얘기
# 숫자가 높을수록 다중공선성이 높다.
# 숫자가 높을수록 나머지 변수들에 영향을 많이 받는다.

vals = [VIF(X, i) for i in range(1, X.shape[1])]
vif = pd.DataFrame({'vif':vals},index=X.columns[1:])
print(vif)
```
- 교호작용 : 두 변수 간의 시너지가 존재하는 경우, 특정 변수가 다른 변수의 영향력에 영향을 줌
![image](https://github.com/user-attachments/assets/4fe64199-f4dd-4717-a3e2-fb680cc18451)

```python
# lstat과 age간의 교호작용을 고려
# -------------------------

# 교호작용은 곱하는 것을 생각하면 된다. eda를 했을 때 유의한 것을 넣으면 됌
X = MS(['lstat', 'age', ('lstat', 'age')]).fit_transform(Boston)
model2 = sm.OLS(y, X)
print(summarize(model2.fit()))
# 결과 lstat이 평균을 높여주는 효과가 age가 클수록 커짐

# lstat에 대해서 다항식 차원을 2차까지 늘림
# --------------------------------
X = MS([poly('lstat', degree=2), 'age']).fit_transform(Boston)
model3 = sm.OLS(y, X)
results3 = model3.fit()
print(summarize(results3))
# 2차식까지 통계적 유의성 확인됨
# 3차를 넣을때는 lstat + lastat제곱 + lastat세제곱
# 절편을 넣어야 한다.


print(anova_lm(results1, results3))
# results1은 lstat에 대한 선형, results3은 lstat에 대한 2차 다항식
# 검정은 다항식을 2차로 쓰는 것이 유의한 지를 검정
```

![image](https://github.com/user-attachments/assets/13ec4b5f-e854-47a3-8122-d1b1b268806f)

1. 위 결과 : 모델 1 (선형 + 상호작용)
   - medv ~ lstat + age + lstat:age
|변수	|계수|	P-value	해석|
|---|---|---|
|lstat|	-1.3921|	0.000	유의미하게 medv에 음의 영향|
|age	|-0.0007	|0.971	유의하지 않음|
|lstat:age|	0.0042|	0.025	상호작용 효과 유의미함|

2. 아래 결과 : 모델 2 (다항회귀 + age)
   - medv ~ poly(lstat, degree=2) + age
|항목|	해석|
|---|---|
|poly(lstat, degree=2)[0]|	2차항, 매우 유의미 (p=0.0)|
|poly(lstat, degree=2)[1]	|1차항, 매우 유의미|
|age|	유의미 (p=0.0)|

- 결과 : 모델 2가 모델 1보다 더 적합하다
- 아래쪽 표 : 두 모델간의 차이가 매우 크다는 것을 나타냄
    - f 통계량 값이 매우 크고 p값도 매우작음
    - 귀무가설 : 두 모델 성능차이 없음
    - 대립가설 : 모델 2가 더 좋다
 
```python
# results3의 결과에 대해서 잔차의 산점도를 그려봄
# 이 지도를 보면 개선된 점을 볼 수 있음
# 전과는 다르게 곡선의 형태가 사라짐
# 변동이 심한 부분은 있지만 곡선때문에 생긴 문제는 해소됌

ax = subplots(figsize=(8,8))[1]
ax.scatter(results3.fittedvalues, results3.resid)
ax.set_xlabel('Fitted value')
ax.set_ylabel('Residual')
ax.axhline(0, c='k', ls='--')
```

![image](https://github.com/user-attachments/assets/da198fce-4774-447c-be74-b452ca33122e)
