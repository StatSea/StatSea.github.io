---
layout: single
title: "[ML] 기계학습 Chap 4 "
categories: [ML]
tags: [ML]
mathjax: true
---
기계학습 Chap 4 정리

---

# 분류
- 반응변수가 질적 / 범주형 변수일때
- 회귀분석과 같이 n개의 훈련자료를 관측했다고 하고 이에 근거하여 분류기 생성
- 훈련자료, 평가자료 둘 다 좋은 성능이 나와야 함
  - 분류에도 언더피팅이 존재한다.

- 환자들을 증상에 따라 1,2,3 의 범주에 할당하는 것
  - 범주들끼리 사칙연산은 하면 안된다.

# 로지스틱 회귀모형
- 금융정보를 바탕으로 채무불이행 여부를 예측하기 위한 모의실험 자료
![image](https://github.com/user-attachments/assets/636e076a-74ca-44b1-b000-0e8b24dd066d)

- 주황색과 하늘색 비율은 비슷하다
- Balance쪽으로 보는게 구별이 잘된다.
- 중요한 변수 : Balance

## 다른 예시
![image](https://github.com/user-attachments/assets/df1e509f-74b2-4529-a659-a3cdf04fc7aa)

- 로지스틱 모형은 y에 대한 직접적 모형화가 아닌 y가 특정 범주에 포함될 확률을 모형화 함
- 반응변수가 두 범주 중 하나로만 결정되는 경우로 binary response임
- 만약 채무 불이행 여부 (default)를 지불잔액(balance)를 이용해서 모형화 하길 원한다면 추정대상은
$$
P(\text{default} = \text{YES} \mid \text{balance}) = p(\text{balance}) \in (0, 1)
$$

- 임계치 : 특정 고객에 대해 p(balance) > 0.5 인 경우 default를 yes로 예측할 수 있다. 하지만 보수적인 결정을 내리고자 하는 경우 p(balance) > 0.1인 고객에 대해 yes 로 예측하는 것도 가능하다. 여기서 0.5, 0.1을 임계치라고 한다.

# 모형
- 관심이 되는 반응변수 값을 보통 1, 나머지를 0이라 하면, 주어진 예측변수 x에 대하여 추정대상은
$$
\Pr(Y = 1 \mid X) = p(X) \in (0, 1)
$$

위 확률을 X에 대해서 선형의로 모형화 한다면? 예측확률이 음수가 되는 경우가 발생하기도 한다.
이로부터 p(x)를 (0,1) 사이의 값으로 예측해주는 모형화가 필요하다.
대표적인 것이 로지스틱 함수이다.

$$
p(X) = \frac{\exp(\beta_0 + \beta_1 X)}{1 + \exp(\beta_0 + \beta_1 X)} < 1
$$

- 이 모형은 간단한 계산을 통해 다음과 같이 표현된다.
- 일어날 확률 대 안 일어날 확률, odds 라고 부른다.
- 베팅을 하는 분야에서 확률 대신 많이 쓰인다.

$$
\frac{p(X)}{1 - p(X)} = \exp(\beta_0 + \beta_1 X)
$$

$$
\log\left( \frac{p(X)}{1 - p(X)} \right) = \beta_0 + \beta_1 X
$$

- 여기에 로그를 붙인다면 logit이 된다.

# 선형모형 vs 로지스틱 모형

![image](https://github.com/user-attachments/assets/7c92ea6f-1f23-4e28-8905-284ea00c2759)

- 선형 회귀 : 예측값이 실수 , 직선으로 표현됌 , x가 1증가할때 y가 얼마나 증가하는지
   - 연속형 수치
- 로지스틱 회귀 : 예측 값이 확률, 결과를 0 또는 1로 분류 , S자 시그모이드 곡선 , x가 1 증가할 때 odds가 몇 배 증가하는지
  - 확률( 이진분류)

### 선형 회귀 모형 (Linear Regression)

$$
Y = \beta_0 + \beta_1 X + \epsilon
$$

- \( Y \): 연속형 종속 변수
- \( \epsilon \): 정규분포를 따르는 오차항

---

### 로지스틱 회귀 모형 (Logistic Regression)

#### ▶ 확률 형태

$$
p(X) = P(Y = 1 \mid X) = \frac{\exp(\beta_0 + \beta_1 X)}{1 + \exp(\beta_0 + \beta_1 X)}
$$

#### ▶ 로그 승산(log-odds) 형태

$$
\log\left( \frac{p(X)}{1 - p(X)} \right) = \beta_0 + \beta_1 X
$$

# 추정
- 반응변수는 범주형 변수이기에 최소제곱법을 사용하는것이 맞지 않을 수 있다.
- 반응변수의 조건부 분포를 이용한 최대우도 추정법으로 추정한다.
- 반응변수가 베르누이 , 이항분포임을 이용하여 가능도 함수를 기술한다.
- 보통 명시적 해가 존재하지 않아 반복을 통한 수치적인 접근을 통하여 추정치를 얻는다.

`-` 로지스틱 회귀 최대우도법 사용

$$
P(Y = y \mid X) = \frac{ \exp(\beta_0 + \beta_1 X)^y }{ 1 + \exp(\beta_0 + \beta_1 X) }
$$

$$
P(Y = y \mid X) =
\left( \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}} \right)^y
\left( \frac{1}{1 + e^{\beta_0 + \beta_1 X}} \right)^{1 - y}
$$

# 예측
- 모수에 대한 추정이 이루어지면 추정치와 주어진 예측 변수를 사용아여 확률을 예측한다.

`-` 예시

balance 로 default 여부를 예측하는 모형
$$
\hat{\beta}_0 = -10.6513, \ \hat{\beta}_1 = 0.0055
$$

지불 잔액 = 1000


![image](https://github.com/user-attachments/assets/db52bdd0-d774-492d-a6db-fc2d44162861)

- 지불 잔액이 1 증가하면, odds 가 exp(0.0055)배 증가하게 된다.
-  0.0055가 기울기 이기 때문이다.
-  값이 0.00576이므로 > 0.5가 안된다... 사용 안하는게 좋아 보인다.
-  지불잔액이 1000인 고객이 연체할 확률 = 0.00576 = 0.576%

# 다중 로지스틱 모형
- 여러 개의 예측 변수를 사용하는 경우도 마찬가지로 확장하여 생각할 수 있다.
- 한 개의 예측 변수를 사용할 때와 여러 개의 예측변수가 동시에 사용될 때 효과의 형태가 다르게 나타날 수도 있다.
- 변수끼리 곱해서 term으로 집어넣을 필요도 있다.

# 다항반응변수에서의 로지스틱 모형
- 반응변수가 셋 이상의 범주를 가질 때, 로지스틱 모형을 확장하여 고려할 수 있다.
- 그러나 다른 대안들의 존재로 인하여 로지스틱 모형은 폭넓게 쓰이지 않는다
- 대표 적인 대안 : 선형판별 분석

# 판별 분석
- 반응변수와 예측변수들의 결합분포에 기반한 방법
- 어떤 그룹에 속하는지를 예측하는 분석 방법
- 관측값이 어떤 집단에 속하는지 판별하는 방법


`-` 중요 장점

- 범주들이 잘 분리되어 있을 때, 로지스틱 모형은 불안정하나 판별분석은 그렇지 않다
- n이 작고 X의 분포가 정규분포에 가까울 때 판별분석의 성능이 로지스틱보다 좋다.
- 다범주 반응변수의 경우 로지스틱에 비해 더 간단하다.
  
![image](https://github.com/user-attachments/assets/7b48a988-f969-4baa-a681-637efe437258)

# 베이즈 정리

- $k$번째 범주로부터 관측된 $X$의 분포를 $f_k(x) \equiv Pr(X = x \mid Y = k)$라고 하고,  
$\pi_k$를 각 범주의 사전확률이라 하면, 베이즈 정리에 의하여 다음을 얻는다.

$$
Pr(Y = k \mid X = x) = p_k(x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^K \pi_l f_l(x)}
$$

![image](https://github.com/user-attachments/assets/37b436e9-bd64-4131-8633-9691bcdaacd0)


# 선형판별분석
$$
p = 1,\quad f_k(x) \text{가 정규분포임을 가정함. 즉, } f_k(x) = \mathcal{N}(\mu_k, \sigma_k^2)
$$

$$
\text{각 범주에 해당하는 분포의 분산은 동일하다고 가정. 즉, } 
\sigma_1^2 = \cdots = \sigma_K^2 = \sigma^2
$$

$$
\text{이 경우, 조건부 확률 } p_k(x) \text{를 최대화 시키는 } k \text{를 찾는 것은}
$$

$$
\delta_k(x) = x \cdot \frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + \log \pi_k
$$

$$
\text{를 최대화하는 } k \text{를 찾는 것과 같음을 보일 수 있다.}
$$

$$
K = 2,\quad \pi_1 = \pi_2 \text{인 경우, 분류를 위한 경계치는}
$$

$$
x = \frac{\mu_1 + \mu_2}{2}
$$

$$
\text{와 같이 간단히 된다.}
$$

![image](https://github.com/user-attachments/assets/25c6daf0-4ce6-466e-b29e-99f6982fdb39)
![image](https://github.com/user-attachments/assets/8203e74b-4659-4eab-b833-de89650f8946)


# 베이즈 분류기로의 근사
$$
p = 1 \text{인 경우, LDA는 다음과 같이 베이즈 분류기를 근사하게 된다.}
$$

$$
\delta_k(x) = x \cdot \frac{\hat{\mu}_k}{\hat{\sigma}^2} - \frac{\hat{\mu}_k^2}{2 \hat{\sigma}^2} + \log \hat{\pi}_k
$$

$$
\hat{\mu}_k = \frac{1}{n_k} \sum_{i : y_i = k} x_i
$$

$$
\hat{\sigma}^2 = \frac{1}{n - K} \sum_{k=1}^{K} \sum_{i : y_i = k} (x_i - \hat{\mu}_k)^2
$$

$$
\hat{\pi}_k = \frac{n_k}{n}
$$

$$
\delta_k(x) \text{를 최대화시키는 범주 } k \text{로 관측치를 할당.}
$$

$$
\text{‘linear’라는 단어는 판별함수 } \delta_k(x) \text{가 } x \text{의 선형함수로 표현된다는 사실로부터 유래함.}
$$
![image](https://github.com/user-attachments/assets/14af9514-a636-43cf-bbfc-1bccf8c5cdd5)

# 선형 판별 분석 (P>1)
$$
\text{예측변수가 다변량인 경우, } k \text{번째 범주에서 예측변수가 다변량 정규분포 } 
N(\mu_k, \Sigma),\ \mu \in \mathbb{R}^p,\ \Sigma : p \times p
\text{ 를 따른다는 가정으로부터 출발하게 됨.}
$$

$$
p = 1 \text{일 때와 마찬가지로 모든 범주에서 공분산 행렬 } \Sigma \text{는 동일하다고 가정함.}
$$

$$
\text{이 경우 판별함수는 다음과 같이 주어지게 됨.}
$$

$$
\delta_k(x) = x^\top \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^\top \Sigma^{-1} \mu_k + \log \pi_k
$$
![image](https://github.com/user-attachments/assets/736caae0-148f-4221-bd63-76653a2ff7b0)
