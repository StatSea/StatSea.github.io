---
layout: single
title: "[ML] 기계학습 Chap 4 코드 정리"
categories: [ML]
tags: [ML]
mathjax: true
---
기계학습 Chap 4 코드 정리

---

# 1. 기본 세팅
```python
!pip install ISLP
!pip install matplotlib

import numpy as np
import pandas as pd
from matplotlib.pyplot import subplots
import statsmodels.api as sm
from ISLP import load_data
from ISLP.models import (ModelSpec as MS,summarize)
from ISLP import confusion_table
from ISLP.models import contrast
from sklearn.discriminant_analysis import  (LinearDiscriminantAnalysis as LDA, QuadraticDiscriminantAnalysis as QDA)
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
```

# 2. Smarket Data 분석
- direction 수치화 한 후 상관행렬 계산
```python
Smarket = load_data('Smarket')
Smarket
print(Smarket.columns)
import copy

Smar = copy.deepcopy(Smarket)
Smar['Direction'] = Smar['Direction'].map({'Up': 1, 'Down': 0})
print(Smar.corr())
plt.plot(Smar['Volume'])
plt.show()

# today말고 나머지는 연속형인 것 같음.
# direction도 연속형 x
# 해석할때 조심해야 한다.

```
![image](https://github.com/user-attachments/assets/a2086773-9715-466a-a737-0e58b4b0de86)

- 각 변수들 간의 상관관계를 본 것이다.
![image](https://github.com/user-attachments/assets/fbbdedaa-e9b2-4cd2-9aa6-52b20aade1e9)
- Smarket 데이터의 volume 변수의 시간 순서대로 그린 그래프

```python
allvars = Smarket .columns.drop(['Today', 'Direction', 'Year'])
design = MS(allvars)
X = design.fit_transform(Smarket)
y = Smarket.Direction == "Up"
print(y)
glm = sm.GLM(y, X, family=sm.families.Binomial())
results = glm.fit()
print(summarize(results))

# 나머지는 필요가 없어 보여서 삭제함
# lag들만 남겼음

# p값을 보면 0.05보다 다 커서 유의한게 아무것도 없는 것을 알 수 있음
```
- 몇개의 변수를 제외하고 문자로 되어있는 direction 변수를 0,1로 변환한다.
- 로지스틱 회귀분석을 적용하였다.

![image](https://github.com/user-attachments/assets/a67d3d2c-06de-45ad-8ddd-329ec53bdfe0)

## 그냥 로지스틱으로 예측해보기
```python
probs = results.predict()
probs[:10]
labels = np.array(['Down']*1250)
labels[probs>0.5] = "Up"
print(confusion_table(labels, Smarket.Direction))
np.mean(labels == Smarket.Direction)

- label이 예측하는거
- 혼동행렬 출력
- 예측 정확도 출력 (평균)
```
- 예측된 확률을 살펴보고 예측확률이 0.5를 넘으면 up으로 예측
- 훈련 데이터의 예측에 대한 혼동행렬 계산
- 예측 정확도 계산
  ![image](https://github.com/user-attachments/assets/fe457024-dfa7-4401-82b2-8012a5833048)

## 훈련데이터로 쪼개서 해보기
```python
train = (Smarket.Year < 2005)
Smarket_train = Smarket.loc[train]
Smarket_test = Smarket.loc[~train]
Smarket_test.shape

X_train, X_test = X.loc[train], X.loc[~train]
y_train, y_test = y.loc[train], y.loc[~train]
glm_train = sm.GLM(y_train, X_train,family=sm.families.Binomial())
results = glm_train.fit()
probs = results.predict(exog=X_test)
D = Smarket.Direction
L_train, L_test = D.loc[train], D.loc[~train]
labels = np.array(['Down']*252)
labels[probs>0.5] = 'Up'
print(confusion_table(labels, L_test))
np.mean(labels != L_test)

# 더 엉망이 됌..
#
```
![image](https://github.com/user-attachments/assets/e14c8853-6f59-4958-a2f6-8b384f97245c)

## 일부 변수만 가지고 로지스틱 회귀분석 진행
```python
model = MS(['Lag1', 'Lag2']).fit(Smarket)
X = model.transform(Smarket)
X_train, X_test = X.loc[train], X.loc[~train]
glm_train = sm.GLM(y_train,
X_train ,
family=sm.families.Binomial())
results = glm_train.fit()
probs = results.predict(exog=X_test)
labels = np.array(['Down']*252)
labels[probs>0.5] = 'Up'
print(confusion_table(labels, L_test))

# 예측 정확도 0.55가 됌
# 변수를 다 사용하는 것보다 몇개를 추리는게 좋아보임

# print(probs[:10]) 으로 확률 확인해보기
```

![image](https://github.com/user-attachments/assets/50580f90-3765-4cd9-8b28-b9e95e749d6f)

- 새로운 데이터가 들어왔을 때 예측도 할 수 있음
```python
newdata = pd.DataFrame({'Lag1':[1.2, 1.5], 'Lag2':[1.1, -0.8]});
newX = model.transform(newdata)
print(results.predict(newX))
```
- 전부 0.5보다 작으므로 0으로 예측

# 3. LDA & QDA
- LDA로 예측변수들이 주어졌을 때, 0과 1의 확률을 계산
- 0과 1의 종류별로 평균벡터와 공유하는 공분산행렬 그리고 각 종에 대한 prior를 보여줌
- 여기에서 prior는 실제 0과 1의 비율을 의미

```python
lda = LDA(store_covariance=True)
XX_train, XX_test = [M.drop(columns=['intercept']) for M in [X_train, X_test]]

a, c = np.unique(L_train, return_counts=True)
print(c/np.sum(c))

lda.fit(XX_train, L_train)
print(lda.means_)
print(lda.covariance_)
print(lda.classes_)
print(lda.priors_)

# lda를 할 때 필요한 것 : 평균, 공분산행렬
# 2차원이라 2개로 나옴
```
![image](https://github.com/user-attachments/assets/daec9a0f-4d56-4d42-8d63-656dc34f18b7)
- up 비율이 더 높다.

## LDA를 이용한 예측 결과 및 혼동행렬
```python
lda_pred = lda.predict(XX_test)
print(lda_pred)
print(confusion_table(lda_pred, L_test))

# 뭔가 결과가 나쁘지 않아 보인다.
```
![image](https://github.com/user-attachments/assets/44e26c03-ac6e-46a2-b20c-a80e0653b905)

## QDA를 사용한 결과
- 여기는 공분산 행렬이 공유되지 않고 0과 1 별로 별도의 공분산 행렬이 추정된다.
```python
qda = QDA(store_covariance=True)
print(qda.fit(XX_train, L_train))

qda.means_, qda.priors_
print(qda.covariance_[0])
print(qda.covariance_[1])

# L트레인은 label값
# 공분산 행렬이 유사해 보임
```
![image](https://github.com/user-attachments/assets/f30f8455-0f83-4230-844b-4c5d087bacdb)

```python
qda_pred = qda.predict(XX_test)
#print(qda_pred)
print(confusion_table(qda_pred, L_test))
np.mean(qda_pred == L_test)

# 결과를 보면 0.599까지 나옴
# lda보다 좋은 결과임을 알 수 있음
```
![image](https://github.com/user-attachments/assets/fe3b2096-10ba-4ebf-9de4-01b58ce6bf49)

# 4. Naive Bayes
- Naive Bayes를 이용한 결과
- 여기에서 theta는 각 종별 평균벡터 var_는 공분산 행렬의 대각값을 의미 ( 각 종별 분산 )

```python
NB = GaussianNB()
rs = NB.fit(XX_train, L_train)
print(rs)
print(NB.class_prior_)
print(NB.theta_) # 각 클래스의 평균
print(NB.var_) # 클래스 원에서 변수 1의 분산, 변수 2의 분산 , 클래스 2에서 변수 1의 분산, 변수 2의 분산
# 공분산행렬이 없음... 각각의 분산이 나옴

```
![image](https://github.com/user-attachments/assets/7fa29c4a-1ae0-4868-afbf-5dac02e57d21)

```python
nb_labels = NB.predict(XX_test)
print(confusion_table(nb_labels , L_test))

# 거의 차이가 없음을 확인할 수 있음.
# down관점에서는 안좋음(0은 잘 못맞춤)
# up관점에서는 좋음 (1은 잘맞춤)
```
![image](https://github.com/user-attachments/assets/5f18a922-d19a-434c-a82e-4fc69b834813)

# 5. KNN classifier

```python
knn1 = KNeighborsClassifier(n_neighbors=1)
knn1.fit(XX_train , L_train)
knn1_pred = knn1.predict(XX_test)
print(confusion_table(knn1_pred , L_test))
knn3 = KNeighborsClassifier(n_neighbors=3)
knn3_pred = knn3.fit(XX_train , L_train).predict(XX_test)
print(confusion_table(knn3_pred , L_test))


# 주변 하나만 볼때와 주변 3개를 볼때가 결과가 조금 다름
# 근데 좋은 결과는 아님..
```

- 데이터를 불러오고 예측을 진행

```python
# data loading
# ------------
Caravan = load_data('Caravan')
Purchase = Caravan.Purchase
Purchase.value_counts()
feature_df = Caravan.drop(columns=['Purchase'])

scaler = StandardScaler(with_mean=True, with_std=True, copy=True)
scaler.fit(feature_df)
# 표준화를 위한 평균과 표준편차를 계산
X_std = scaler.transform(feature_df)
# 표준화를 진행

feature_std = pd.DataFrame(X_std , columns=feature_df.columns)
feature_std.std()
```

- knn 으로 분류하고 테스트 데이터에서 성능 확인
```python
(X_train, X_test, y_train, y_test) = train_test_split(feature_std, Purchase, test_size=1000, random_state=0)
knn1 = KNeighborsClassifier(n_neighbors=1)
knn1_pred = knn1.fit(X_train , y_train).predict(X_test)

np.mean(y_test != knn1_pred), np.mean(y_test != "No")
print(confusion_table(knn1_pred , y_test))

# no는 잘 맞추고 yes는 잘 못맞춤
# 데이터에 따라서 결과가 달라지는 것을 확인할 수 있음.
```

## KNN 에서 k의 값에 따른 성능 비교
```python
for K in range(1,6):
  knn = KNeighborsClassifier(n_neighbors=K)
  knn_pred = knn.fit(X_train , y_train).predict(X_test)
  C = confusion_table(knn_pred, y_test)
  templ = ('K={0:d}: # predicted to rent: {1:>2},' +' # who did rent {2:d}, accuracy {3:.1%}')
  pred = C.loc['Yes'].sum()
  did_rent = C.loc['Yes','Yes']
  print(templ.format(K, pred, did_rent, did_rent / pred))
  # rent를 한 사람을 맞춘 것에 대한 정확도임

  # k를 바꾸면서 했을때 정확도
  # 민감도에 대해서만 보았을 경우

```

![image](https://github.com/user-attachments/assets/a718a816-384c-4a51-9ed9-430d59631aa6)

- k=1로 모델 학습 및 예측
  - 62명을 렌트할 것이라 예측
  - 실제로 렌트한 사람은 9명
  - 정확도 = 9/62 = 14.5%

## 로지스틱 회귀 분석과 비교
```python
logit = LogisticRegression(C=1e10 , solver='liblinear')
logit.fit(X_train , y_train)
logit_pred = logit.predict_proba(X_test)
logit_labels = np.where(logit_pred[:,1] > 5, 'Yes', 'No')
print(confusion_table(logit_labels , y_test))
logit_labels = np.where(logit_pred[:,1]>0.25, 'Yes', 'No')
print(confusion_table(logit_labels , y_test))

# 기준을 다르게 했을 경우 값이 달라짐
# 0.5와 0.25일때 결과값을 봄
# yes로 판정하는 기준을 다르게 해본 것

# 근데 결과를 보니 상태가 좀 심각함..
# 그래도 하나도 못맞춘 것보다 몇개라도 맞추게 됌
```
![image](https://github.com/user-attachments/assets/f1daf4cf-7b96-4860-8155-4bd411697f11)


