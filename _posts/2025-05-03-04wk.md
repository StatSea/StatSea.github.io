---
layout: single
title: "[DL] 딥러닝 04wk"
categories: [DL]
tags: [DL]
mathjax: true
---

딥러닝 04wk 이해하기

---

# 기본 세팅
```python
import torch
import matplotlib.pyplot as plt
import pandas as pd
plt.rcParams['figure.figsize'] = (4.5, 3.0)
```

---

# 1. 꺾인 직선을 만드는 방법
- 회귀(카페예제): yhat=직선=linr(x), 정규분포, MSEloss
- 로지스틱(스펙과취업): yhat=곡선=sig(직선)=sig(linr(x)), 베르누이, BCELoss
- 이름없음(스펙의역설): yhat=꺽인곡선=sig(꺽인직선)=sig(??), 베르누이, BCELOss

`-` 로지스틱의 한계를 극복하기 위해서는 시그모이드를 취하기 전에 꺾인 그래프 모양을 만드는 기술이 필요함

- 벡터 x가정
```python
x = torch.linspace(-1,1,1001).reshape(-1,1)
x
```

`-` 목표 : 아래와 같은 벡터 y를 만들어 보기

$${\bf y} = [y_1,y_2,\dots,y_{n}]^\top, \quad y_i = \begin{cases} 9x_i +4.5& x_i <0 \\ -4.5x_i + 4.5& x_i >0 \end{cases}$$



## 방법 1 - 수식 그대로 구현

```python
plt.plot(x,9*x+4.5,color="blue",alpha=0.1)
plt.plot(x[x<0], (9*x+4.5)[x<0],color="blue")
plt.plot(x,-4.5*x+4.5,color="orange",alpha=0.1)
plt.plot(x[x>0], (-4.5*x+4.5)[x>0],color="orange")
```
![image](https://github.com/user-attachments/assets/ccc8a3c9-ea0d-4bab-b03c-3ecd288a56f7)

```python
y = x*0
y[x<0] = (9*x+4.5)[x<0]
y[x>0] = (-4.5*x+4.5)[x>0]
plt.plot(x,y)
```
![image](https://github.com/user-attachments/assets/bf839e39-de99-49f7-9486-8f447b2ef577)

## 방법2 - 렐루 이용

```python
relu = torch.nn.ReLU()
#plt.plot(x,-4.5*relu(x),color="red")
#plt.plot(x,-9*relu(-x),color="blue")
y = -4.5*relu(x) + -9*relu(-x) + 4.5
plt.plot(x,y)
```

+ 중간 과정을 좀 더 시각화... (강의 때 안함)
```python
fig = plt.figure(figsize=(6, 4))
spec = fig.add_gridspec(4, 3)
ax1 = fig.add_subplot(spec[:2,0]); ax1.set_title(r'$x$'); ax1.set_ylim(-1,1)
ax2 = fig.add_subplot(spec[2:,0]); ax2.set_title(r'$-x$'); ax2.set_ylim(-1,1)
ax3 = fig.add_subplot(spec[:2,1]); ax3.set_title(r'$relu(x)$'); ax3.set_ylim(-1,1)
ax4 = fig.add_subplot(spec[2:,1]); ax4.set_title(r'$relu(-x)$'); ax4.set_ylim(-1,1)
ax5 = fig.add_subplot(spec[1:3,2]); ax5.set_title(r'$-4.5 relu(x)-9 relu(-x)+4.5$')
#---#
ax1.plot(x,'--',color='C0')
ax2.plot(-x,'--',color='C1')
ax3.plot(relu(x),'--',color='C0')
ax4.plot(relu(-x),'--',color='C1')
ax5.plot(-4.5*relu(x)-9*relu(-x)+4.5,'--',color='C2')
fig.tight_layout()
```

![image](https://github.com/user-attachments/assets/03fe8729-a55a-46c9-82eb-a42d8ef0b049)


## 방법 3 - 렐루의 브로드 캐스팅 활용
- 우리가 하고 싶은 것
```python
# y = -4.5*relu(x) + -9*relu(-x) + 4.5
```
- 아래와 같은 아이디어로 y를 계산해도 된다.

1. x, relu 준비
2. u = [x -x]
3. v = relu(u) = [relu(x), relu(-x)] = [v1 v2]
4. y = -4.5\*v1 + -9\*v2 + 4.5

```python
u = torch.concat([x,-x],axis=1)
v = relu(u)
v1 = v[:,[0]]
v2 = v[:,[1]]
y = -4.5*v1 -9*v2 + 4.5
plt.plot(x,y)
```
![image](https://github.com/user-attachments/assets/6dbcc9fd-c928-4c5a-bb47-0e6256770d6b)

## 방법 4 - y = linr(v)
```python
# y = -4.5*v1 + -9*v2 + 4.5 = [v1 v2] @ [[-4.5],[-9]] + 4.5
# y = -4 + 3*x = [1 x] @ [[-4],[3]]
```

```python
x
u = torch.concat([x,-x],axis=1)
v = relu(u)
y = v @ torch.tensor([[-4.5],[-9]]) + 4.5
```
![image](https://github.com/user-attachments/assets/46355e56-7bc6-4abb-88a6-458a4d47c40e)


## 방법 5 - u=linr(x)
```python
# x
# u = torch.concat([x,-x],axis=1)
# v = relu(u)
# y = v @ torch.tensor([[-4.5],[-9]]) + 4.5
```

- 위에랑 똑같은 거 같은데..


## 방법 6 - torch.nn.Linear()를 이용
```python
# x
# u = x @ torch.tensor([[1.0, -1.0]]) = l1(x)
# v = relu(u) = a1(u)
# y = v @ torch.tensor([[-4.5],[-9]]) + 4.5 = l2(v)
```

```python
# u = l1(x) # l1은 x->u인 선형변환: (n,1) -> (n,2) 인 선형변환
l1 = torch.nn.Linear(1,2,bias=False)
l1.weight.data = torch.tensor([[1.0, -1.0]]).T
a1 = relu
l2 = torch.nn.Linear(2,1,bias=True)
l2.weight.data = torch.tensor([[-4.5],[-9]]).T
l2.bias.data = torch.tensor([4.5])
#---#
x
u = l1(x)
v = a1(u)
y = l2(v)
```

1. ![image](https://github.com/user-attachments/assets/eaa13e4b-e835-401a-ac96-753f3680d045)

2. ![image](https://github.com/user-attachments/assets/1c749703-689b-4bfd-a220-ad230a7be26f)
  
3. ![image](https://github.com/user-attachments/assets/3aefe410-160a-40a6-8f9f-4f0c63985c73)

- 2번은 절댓값과 비슷한 동작임

```python
plt.plot(x,y.data)
pwlinr = torch.nn.Sequential(l1,a1,l2)
plt.plot(x,pwlinr(x).data)
```
![image](https://github.com/user-attachments/assets/6ae9c5ec-41ca-4347-84f8-178832a63fbc)

- 이 2개가 동일한 동작을 함
- y.data는 파이토치 텐서의 값을 넘파이처럼 가져올 수 있게 해줌
- pwlinr는 신경망 전체를 하나의 시퀀스 모델로 묶는 것
- pwlinr(x)만으로 전체 forward 계산이 수행될 수 있음
- y.data == pwlinr(x)인 셈 !

# 2. 스펙의 역설 적합
- 데이터 정리
```python
df = pd.read_csv("https://raw.githubusercontent.com/guebin/DL2025/main/posts/ironyofspec.csv")

x = torch.tensor(df.x).float().reshape(-1,1)
y = torch.tensor(df.y).float().reshape(-1,1)
prob = torch.tensor(df.prob).float().reshape(-1,1)

plt.plot(x,y,'.',alpha=0.03)
plt.plot(x,prob,'--')
```

![image](https://github.com/user-attachments/assets/d6d8f7ba-ff10-47d1-a48b-e43744cbdc79)

## step1 : 네트워크를 어떻게 만들까? = 아키텍처를 어떻게 만들까? = 모델링

- 2층 신경망 계층 정의

$$\underset{(n,1)}{\bf X} \overset{l_1}{\to} \underset{(n,2)}{\boldsymbol u^{(1)}} \overset{a_1}{\to} \underset{(n,2)}{\boldsymbol v^{(1)}} \overset{l_1}{\to} \underset{(n,1)}{\boldsymbol u^{(2)}} \overset{a_2}{\to} \underset{(n,1)}{\boldsymbol v^{(2)}}=\underset{(n,1)}{\hat{\boldsymbol y}}$$

- $l_1$: `torch.nn.Linear(1,2,bias=False)`
- $a_1$: `torch.nn.ReLU()`
- $l_2$: `torch.nn.Linear(2,1,bias=True)`
- $a_2$: `torch.nn.Sigmoid()`

- 중간에 차원을 늘리고 비선형 함수를 추가하면 복잡한 함수들을 근사할 수 있다.
- 1->2 : 여러 특징 추출하기
- 2->1 : 중요한 정보를 하나로 요약하기
- 은닉층은 항상 출력보다 큰 차원을 가진다.

## step1~4
```python
torch.manual_seed(1)
net = torch.nn.Sequential(
    torch.nn.Linear(1,2,bias=False),
    torch.nn.ReLU(),
    torch.nn.Linear(2,1,bias=True),
    torch.nn.Sigmoid()
)
loss_fn = torch.nn.BCELoss()
optimizr = torch.optim.Adam(net.parameters())
```

```python
for epoc in range(5000):
    ## step1
    yhat = net(x)
    ## step2
    loss = loss_fn(yhat,y)
    ## step3
    loss.backward()
    ## step4
    optimizr.step()
    optimizr.zero_grad()
```

```python
plt.plot(x,y,'.',alpha=0.03)
plt.plot(x,prob,'--')
plt.plot(x,yhat.data,'--')
```

- 이걸 한번 더 반복할 경우 이렇게 출력된다.
![image](https://github.com/user-attachments/assets/ca69d80a-827e-4807-8b7a-fd34062417ee)


- 중간과정을 확인하는 경우
```python
plt.plot(x, net[3](net[2](net[1](net[0](x)))).data)
```

![image](https://github.com/user-attachments/assets/a820a45a-a33f-49de-adb2-e184deeef78b)
