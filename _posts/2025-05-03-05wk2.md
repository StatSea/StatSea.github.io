---
layout: single
title: "[DL] 딥러닝 05wk2"
categories: [DL]
tags: [DL]
mathjax: true
---

딥러닝 05wk2 이해하기

---

# 기본 세팅

```python
import torch
import matplotlib.pyplot as plt

plt.rcParams['figure.figsize'] = (4.5, 3.0)
```

# 1. 신경망의 표현

신경망의 표현: ${\bf X} \to \hat{\bf y}$ 로 가는 과정을 그림으로 표현

## A. 로지스틱

$$\underset{(n,1)}{\bf X} \overset{l_1}{\to} \underset{(n,1)}{\boldsymbol u^{(1)}} \overset{sig}{\to} \underset{(n,1)}{\boldsymbol v^{(1)}} =\underset{(n,1)}{\hat{\bf y}}$$

`-` 모든 observation과 가중치를 명시한 버전

![image](https://github.com/user-attachments/assets/7fa23ef7-7bb1-405f-8a73-b43bcfd15891)

- 단점 : 똑같은 그림의 반복이 너무 많음

`-` observation 반복을 생략한 버전들

**(표현2)** 모든 $i$에 대하여 아래의 그림을 반복한다고 하면 (표현1)과 같다.
![image](https://github.com/user-attachments/assets/12946d0b-dc40-43ce-9fbf-945725e5a778)

**(표현3)** 그런데 (표현2)에서 아래와 같이 $x_i$, $y_i$ 대신에 간단히 $x$, $y$로 쓰는 경우도 많음
![image](https://github.com/user-attachments/assets/d650609b-5e8b-457c-8ace-f0f7ca417f22)

`-` 1을 생략한 버전들

**(표현4)** bais=False 대신에 bias=True를 주면 1을 생략할 수 있음
![image](https://github.com/user-attachments/assets/08cdad4e-ba45-4c37-9d53-e7ccdf93486f)

**(표현4의 수정)** $\hat{w}_1$대신에 $\hat{w}$를 쓰는 것이 더 자연스러움
![image](https://github.com/user-attachments/assets/7cb6a400-67df-4576-94c8-a4538ccd16d0)

**(표현5)** 선형변환의 결과는 아래와 같이 $u$로 표현하기도 한다.

![image](https://github.com/user-attachments/assets/ccceaf2a-3fb9-4daf-b642-7cc8fe9434c9)


## B. 스펙의 역설

$$\underset{(n,1)}{\bf X} \overset{l_1}{\to} \underset{(n,2)}{\boldsymbol u^{(1)}} \overset{relu}{\to} \underset{(n,2)}{\boldsymbol v^{(1)}} \overset{l_2}{\to} \underset{(n,1)}{\boldsymbol u^{(2)}} \overset{sig}{\to} \underset{(n,1)}{\boldsymbol v^{(2)}} =\underset{(n,1)}{\hat{\bf y}}$$

- 코드로 표현한다면
```python
torch.nn.Sequential(
    torch.nn.Linear(in_features=1,out_features=2),
    torch.nn.ReLU(),
    torch.nn.Linear(in_features=2,out_features=1),
    torch.nn.Sigmoid()
)
```

- 강의 노트의 표현
![image](https://github.com/user-attachments/assets/4d563c8b-acf6-410b-bfa1-8ca7c8da9f1b)

- 좀 더 일반화된 표현
![image](https://github.com/user-attachments/assets/b6ce5e82-79a1-44a5-acab-66bafe0bdde7)


`*` Layer의 개념: ${\bf X}$에서 $\hat{\boldsymbol y}$로 가는 과정은 "선형변환+비선형변환"이 반복되는 구조이다. "선형변환+비선형변환"을 하나의 세트로 보면 아래와 같이 표현할 수 있다.

- $\underset{(n,1)}{\bf X}  \overset{l_1}{\to} \left( \underset{(n,2)}{\boldsymbol u^{(1)}} \overset{relu}{\to} \underset{(n,2)}{\boldsymbol v^{(1)}} \right) \overset{l_2}{\to} \left(\underset{(n,1)}{\boldsymbol u^{(2)}} \overset{sig}{\to} \underset{(n,1)}{\boldsymbol v^{(2)}}\right), \quad  \underset{(n,1)}{\boldsymbol v^{(2)}}=\underset{(n,1)}{net({\bf X})}=\underset{(n,1)}{\hat{\bf y}}$

- 이것을 다이어 그램으로 표현한다면 다음과 같다.
( 선형 + 비선형을 하나의 layer로 묶은 표현)

![image](https://github.com/user-attachments/assets/8a3617e2-f9ea-478a-9e24-5fe0e255cceb)

- 레이어를 세는 방법 : 학습 가능한 파라미터가 몇층으로 있는가? 위의 예제는 2이다.
- hidden 레이어를 세는 방법 : layer 수 - 1 , 위의 예제에서는 1이다.


```Python
## 예시1 -- 2층 (히든레이어는 1층)
torch.nn.Sequential(
    torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층
    torch.nn.ReLU(),
    torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층
)
```

```Python
## 예시2 -- 2층 (히든레이어는 1층)
torch.nn.Sequential(
    torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층
    torch.nn.ReLU(),
    torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층
    torch.nn.Sigmoid(),
)
```

```Python
## 예시3 -- 1층 (히든레이어는 없음!!)
torch.nn.Sequential(
    torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층
)
```

```Python
## 예시4 -- 1층 (히든레이어는 없음!!)
torch.nn.Sequential(
    torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층
    torch.nn.Sigmoid()
)
```

```Python
## 예시5 -- 3층 (히든레이어는 2층)
torch.nn.Sequential(
    torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층
    torch.nn.Sigmoid()
    torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층
    torch.nn.Sigmoid()
    torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층    
)
```

```Python
## 예시6 -- 3층 (히든레이어는 2층)
torch.nn.Sequential(
    torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층
    torch.nn.ReLU()
    torch.nn.Dropout(??)
    torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층
    torch.nn.ReLU()
    torch.nn.Dropout(??)
    torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층  
    torch.nn.Sigmoid()
)
```

- 문헌에 따라서 레이어 + 1을 한 경우가 있는데 이것은 잘못된 것.
- 히든 레이어의 수는 예전이나 지금이나 동일하므로 히든만 세면 혼돈이 없다.


`*` node의 개념: $u\to v$로 가는 쌍을 간단히 노드라는 개념을 이용하여 나타낼 수 있음.

(노드의 개념이 포함된 그림)

![image](https://github.com/user-attachments/assets/09428875-4418-4bfc-be2a-414e82f2f5a1)

- 여기에서 노드의 숫자 = feature의 숫자와 같이 이해할 수 있다.

![image](https://github.com/user-attachments/assets/9f573e10-f1d2-4dba-8bfb-ae6ad189e685)

> 다이어그램의 표현방식은 교재마다 달라서 모든 예시를 달달 외울 필요는 없습니다. 다만 임의의 다이어그램을 보고 대응하는 네트워크를 pytorch로 구현하는 능력은 매우 중요합니다.




## C. MNIST

$$\underset{(n,784)}{\bf X} \overset{l_1}{\to} \underset{(n,32)}{\boldsymbol u^{(1)}} \overset{relu}{\to} \underset{(n,32)}{\boldsymbol v^{(1)}} \overset{l_1}{\to} \underset{(n,1)}{\boldsymbol u^{(2)}} \overset{sig}{\to} \underset{(n,1)}{\boldsymbol v^{(2)}}=\underset{(n,1)}{\hat{\boldsymbol y}}$$


(다이어그램 표현)
![image](https://github.com/user-attachments/assets/3dce9e2a-1361-4b16-b983-267f8df4d83a)

- layer0,1,2 대신에 input layer, hidden layer, output layer로 표현함

- 위의 다이어그램에 대응하는 코드

```python
net = torch.nn.Sequential(
    torch.nn.Linear(in_features=28*28*1,out_features=32),
    torch.nn.ReLU(),
    torch.nn.Linear(in_features=32,out_features=1),
    torch.nn.Sigmoid()
)
```

# 2. CPU vs GPU

- 파이토치에서 gpu를 쓰는 방법을 알아보자

## GPU 사용방법

- cpu 연산이 가능한 메모리에 데이터 저장

```python
torch.manual_seed(43052)
x_cpu = torch.tensor([0.0,0.1,0.2]).reshape(-1,1)
y_cpu = torch.tensor([0.0,0.2,0.4]).reshape(-1,1)
net_cpu = torch.nn.Linear(1,1)

net_cpu(x_cpu)
```

- gpu 연산이 가능한 메모리에 데이터 저장

```python
!nvidia-smi # before
```
![image](https://github.com/user-attachments/assets/c30edddd-6b48-494d-a3db-ae10e503b91f)


```python
torch.manual_seed(43052)
x_gpu = x_cpu.to("cuda:0")
y_gpu = y_cpu.to("cuda:0")
net_gpu = torch.nn.Linear(1,1).to("cuda:0")
```

```python
!nvidia-smi
```
![image](https://github.com/user-attachments/assets/ee05322e-2c7c-4ecb-96e2-c418143ec26a)


- gpu에 메모리를 올리면 gpu 메모리가 점유됨을 알 수 있음

- cpu 혹은 gpu 연산이 가능한 메모리에 저장된 값들을 확인 : gpu는 cuda:0이라고 되어있음
- gpu는 gpu끼리 연산이 가능하고 cpu는 cpu끼리 연산이 가능함 

```python
net_gpu(x_cpu) # 이 경우는 안된다.
```

## 시간 측정 : cpu vs gpu (500 nodes)

- cpu(500 nodes)

```python
torch.manual_seed(5)
x=torch.linspace(0,1,100).reshape(-1,1)
y=torch.randn(100).reshape(-1,1)*0.01
#---#
net = torch.nn.Sequential(
    torch.nn.Linear(1,500),
    torch.nn.ReLU(),
    torch.nn.Linear(500,1)
)
loss_fn = torch.nn.MSELoss()
optimizr = torch.optim.Adam(net.parameters())
#---#
t1 = time.time()
for epoc in range(1000):
    # 1
    yhat = net(x)
    # 2
    loss = loss_fn(yhat,y)
    # 3
    loss.backward()
    # 4
    optimizr.step()
    optimizr.zero_grad()
t2 = time.time()
t2-t1
```

- 0.36초가 걸린다.

- gpu(500 nodes)

```python
torch.manual_seed(5)
x=torch.linspace(0,1,100).reshape(-1,1).to("cuda:0")
y=(torch.randn(100).reshape(-1,1)*0.01).to("cuda:0")
#---#
net = torch.nn.Sequential(
    torch.nn.Linear(1,500),
    torch.nn.ReLU(),
    torch.nn.Linear(500,1)
).to("cuda:0")
loss_fn = torch.nn.MSELoss()
optimizr = torch.optim.Adam(net.parameters())
#---#
t1 = time.time()
for epoc in range(1000):
    # 1
    yhat = net(x)
    # 2
    loss = loss_fn(yhat,y)
    # 3
    loss.backward()
    # 4
    optimizr.step()
    optimizr.zero_grad()
t2 = time.time()
t2-t1
```

- 0.58 초가 걸린다.
- cpu가 더 빠르게 나온다..?
- node가 200,000 인 경우 gpu가 80배 더 빠르다
- 이런 차이가 나는 이유 : 연산을 하는 주체는 코어인데 cpu는 수가 적지만 일을 잘하는 코어들을 가지고 있고 gpu는 일은 못하지만 다수의 코어를 가지고 있기 때문이다.

## 주의점 

- tensor인 경우

```python
x = torch.tensor([1,2,3])
x.to("cuda:0"), x
```

- net인 경우

```python
net = torch.nn.Linear(1,1).to("cuda:0")
net.weight, net.bias
```

# 3. 확률적 경사하강법

## A. 의문 : 좀 이상하지 않아요?
- gpu 가 너무 비쌈..
- 우리가 분석하는 데이터

```python
x = torch.linspace(-10,10,100000).reshape(-1,1)
eps = torch.randn(100000).reshape(-1,1)
y = x*2 + eps

plt.plot(x,y,'.',alpha=0.05)
plt.plot(x,2*x,'--')
```
![image](https://github.com/user-attachments/assets/ccad9994-ff8a-47d3-b69e-4b6e5e1fda74)

`-` 데이터의 크기가 커지는 순간 `x.to("cuda:0")`, `y.to("cuda:0")` 쓰면 난리나겠는걸? $\to$ 이런식이면 GPU를 이용하여 아무런 분석도 못할것 같은데?? 뭔가 좀 이상한데??

`-` 아이디어: 데이터를 100개중에 1개 꼴로만 쓰면 어떨까?

```python
plt.plot(x[::100],y[::100],'o',alpha=0.05)
plt.plot(x,2*x,'--')
```
![image](https://github.com/user-attachments/assets/05106be3-990e-43bf-a393-b26b722991bb)

- 대충 이걸로만 적합해도 충분히 정확할 것 같은데?

## B. x,y 데이터를 굳이 모두 gpu에 넘길 필요가 있나?

`-` 데이터셋을 짝홀로 나누어서 번갈아가면서 GPU에 올렸다 내렸다하면 안되나?

`-` 아래의 알고리즘을 생각해보자.

1. 데이터를 반으로 나눈다.
2. 짝수obs의 x,y 그리고 net의 모든 파라메터를 GPU에 올린다.
3. yhat, loss, grad, update 수행
4. 짝수obs의 x,y를 GPU메모리에서 내린다. 그리고 홀수obs의 x,y를 GPU메모리에 올린다.
5. yhat, loss, grad, update 수행
6. 홀수obs의 x,y를 GPU메모리에서 내린다. 그리고 짝수obs의 x,y를 GPU메모리에 올린다.
7. 반복

> 이러면 되는거아니야???? ---> 맞아요


## C. 경사하강법 , 확률적 경사 하강법, 미니배치 경사하강법

10개의 샘플이 있다고 가정. $\{(x_i,y_i)\}_{i=1}^{10}$

`# ver1` --  모든 샘플을 이용하여 slope 계산

(epoch 1) $loss=\sum_{i=1}^{10}(y_i-\hat{w}_0-\hat{w}_1x_i)^2 \to slope  \to update$
(epoch 2) $loss=\sum_{i=1}^{10}(y_i-\hat{w}_0-\hat{w}_1x_i)^2 \to slope  \to update$

> 우리가 항상 이렇게 함

`# ver2` -- 하나의 샘플만을 이용하여 slope 계산

(epoch 1)

- $loss=(y_1-\hat{w}_0-\hat{w}_1x_1)^2 \to slope \to update$
- $loss=(y_2-\hat{w}_0-\hat{w}_1x_2)^2 \to slope \to update$
- ...
- $loss=(y_{10}-\hat{w}_0-\hat{w}_1x_{10})^2  \to  slope  \to  update$

(epoch 2)

- $loss=(y_1-\hat{w}_0-\hat{w}_1x_1)^2  \to slope  \to  update$
- $loss=(y_2-\hat{w}_0-\hat{w}_1x_2)^2  \to slope  \to  update$
- ...
- $loss=(y_{10}-\hat{w}_0-\hat{w}_1x_{10})^2  \to  slope  \to  update$


`# ver3` -- $m (\leq n)$ 개의 샘플을 이용하여 slope 계산

$m=3$이라고 하자.


(epoch 1)

- $loss=\sum_{i=1}^{3}(y_i-\hat{w}_0-\hat{w}_1x_i)^2  \to  slope  \to  update$
- $loss=\sum_{i=4}^{6}(y_i-\hat{w}_0-\hat{w}_1x_i)^2  \to  slope  \to  update$
- $loss=\sum_{i=7}^{9}(y_i-\hat{w}_0-\hat{w}_1x_i)^2  \to  slope  \to  update$
- $loss=(y_{10}-\hat{w}_0-\hat{w}_1x_{10})^2  \to  slope  \to  update$


(epoch 2)

- $loss=\sum_{i=1}^{3}(y_i-\hat{w}_0-\hat{w}_1x_i)^2  \to  slope  \to  update$
- $loss=\sum_{i=4}^{6}(y_i-\hat{w}_0-\hat{w}_1x_i)^2  \to  slope  \to  update$
- $loss=\sum_{i=7}^{9}(y_i-\hat{w}_0-\hat{w}_1x_i)^2  \to  slope  \to  update$
- $loss=(y_{10}-\hat{w}_0-\hat{w}_1x_{10})^2  \to  slope  \to  update$



## D. 용어의 정리

**옛날**

`-` ver1(모든): gradient descent, batch gradient descent

`-` ver2(하나만): stochastic gradient descent

`-` ver3(몇개만): mini-batch gradient descent, mini-batch stochastic gradient descent


**요즘**

`-` ver1(모든): gradient descent

`-` ver2(하나만): stochastic gradient descent with batch size = 1

`-` **ver3(몇개만): stochastic gradient descent**

## E. Dataset(`ds`), DataLoader(`dl`)

> 취지는 알겠으나, C의 과정을 실제 구현하려면 진짜 어려움.. (입코딩과 손코딩의 차이) --> 이걸 해결하기 위해서 파이토치에서는 DataLoader라는 오브젝트를 준비했음!

- 데이터

```python
x=torch.tensor(range(10)).float().reshape(-1,1)
y=torch.tensor([1.0]*5+[0.0]*5).reshape(-1,1)
torch.concat([x,y],axis=1)
```

- ds 오브젝트

```python
ds = torch.utils.data.TensorDataset(x,y)
ds
```

- <torch.utils.data.dataset.TensorDataset at 0x750d76514d30>

```python
ds.tensors
# 생긴건 ds.tensors = (x,y) 임
```
![image](https://github.com/user-attachments/assets/5b012da1-77f6-4d88-b7ee-22d509c9ebf5)

```python
ds[0],(x,y)[0] # (x,y) 튜플자체는 아님.. 인덱싱이 다르게 동작
```
![image](https://github.com/user-attachments/assets/5ebb8834-02c2-412a-87a7-4d3988321424)


- dl 오브젝트

```python
dl = torch.utils.data.DataLoader(ds, batch_size=3)
```

```python
for x_mbatch,y_mbatch in dl:
    print(f"x_mini_batch:{x_mbatch.tolist()} \t y_mini_batch:{y_mbatch.tolist()}")
```

![image](https://github.com/user-attachments/assets/d0fe4f92-04ec-49da-b231-bffb54a42aed)

- 마지막 관측치는 뭔데 단독으로 업데이트 하지? -> shuffle true 같이 자잘한 옵션도 있다.

```python
dl = torch.utils.data.DataLoader(ds,batch_size=3,shuffle=True)
for x_mbatch,y_mbatch in dl:
    print(f"x_mini_batch:{x_mbatch.tolist()} \t y_mini_batch:{y_mbatch.tolist()}")
```
![image](https://github.com/user-attachments/assets/4195681d-1cb3-4922-8e98-d63750d1b6b9)

## F. 성능체크

- 목표 : 확률적 경사하강법과 그냥 경사하강법의 성능을 동일 반복횟수로 비교해보자.

- mnist 자료를 그냥 경사하강법으로 적합
```python
train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)
to_tensor = torchvision.transforms.ToTensor()
X0 = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==0])
X1 = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==1])
X = torch.concat([X0,X1],axis=0).reshape(-1,784)
y = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)

torch.manual_seed(1)
net = torch.nn.Sequential(
    torch.nn.Linear(784,32),
    torch.nn.ReLU(),
    torch.nn.Linear(32,1),
    torch.nn.Sigmoid()
)
loss_fn = torch.nn.BCELoss()
optimizr = torch.optim.SGD(net.parameters())

for epoc in range(700):
    # step1
    yhat = net(X)
    # step2
    loss = loss_fn(yhat,y)
    # step3
    loss.backward()
    # step4
    optimizr.step()
    optimizr.zero_grad()

((yhat > 0.5) ==  y).float().mean()
```

- tensor(0.9953)

- mnist 자료를 확률적 경사하강법으로 적합해보자 -- 미니배치 쓰는 학습

```python
# train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)
# to_tensor = torchvision.transforms.ToTensor()
# X0 = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==0])
# X1 = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==1])
# X = torch.concat([X0,X1],axis=0).reshape(-1,784)
# y = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)
ds = torch.utils.data.TensorDataset(X,y)
dl = torch.utils.data.DataLoader(ds,batch_size=2048)

len(X)/2048
```

- 6.18이 나오므로 batchsize가 2048이라면 한 epoch당 7회 update가 필요함

```python
torch.manual_seed(1)
net = torch.nn.Sequential(
    torch.nn.Linear(784,32),
    torch.nn.ReLU(),
    torch.nn.Linear(32,1),
    torch.nn.Sigmoid()
)
loss_fn = torch.nn.BCELoss()
optimizr = torch.optim.SGD(net.parameters())

for epoc in range(100):
    for xm,ym in dl:
        # step1
        ym_hat = net(xm)
        # step2
        loss = loss_fn(ym_hat,ym)
        # step3
        loss.backward()
        # step4
        optimizr.step()
        optimizr.zero_grad()

((net(X) > 0.5) ==  y).float().mean()
```

- tensor(0.9931)
