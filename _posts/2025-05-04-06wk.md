---
layout: single
title: "[DL] 딥러닝 06wk "
categories: [DL]
tags: [DL]
mathjax: true
---
딥러닝 06wk 이해하기

# 기본 세팅
```python
import torch
import torchvision
import matplotlib.pyplot as plt

plt.rcParams['figure.figsize'] = (4.5, 3.0)
```
---

## 복습
---
- 시멘코 정리 - 다 맞출 수 있다. (train)
- 오버피팅 - 그게 의미가 없을텐데.. (test)
- 드랍아웃 - 대충 학습하면 오히려 좋을수도 ... -> 랜덤 포레스트 
---
- gpu --> 너무 비싸
- 확률적 경사하강법
- 돈이 없어서 만든게 아니다... 알고리즘 자체에 장점이 있다.
- 데이터를 조금씩만 쓰면서 updata .. -> 대충 하는 느낌 -> 오버핏을 좀 줄여줌
- 글로벌 min, local min -> 아담은 잘 빠진다. 하지만 얘는 잘 빠져나감

-- 오늘 할거 : train/test 데이터에 미니배치 , gpu, 경사하강법, 드랍아웃


# 1. 데이터분석 코딩패턴
## A. 일반적인 train/test 세팅

`-` step 1 : 데이터 정리

```python
train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)
test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True)
to_tensor = torchvision.transforms.ToTensor()
X0 = torch.stack([to_tensor(img) for img, lbl in train_dataset if lbl==0])
X1 = torch.stack([to_tensor(img) for img, lbl in train_dataset if lbl==1])
X = torch.concat([X0,X1],axis=0).reshape(-1,784)
y = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)
XX0 = torch.stack([to_tensor(img) for img, lbl in test_dataset if lbl==0])
XX1 = torch.stack([to_tensor(img) for img, lbl in test_dataset if lbl==1])
XX = torch.concat([XX0,XX1],axis=0).reshape(-1,784)
yy = torch.tensor([0.0]*len(XX0) + [1.0]*len(XX1)).reshape(-1,1)
```

```python
XX.shape , y.shape, X.shape, yy.shape
```
![image](https://github.com/user-attachments/assets/0df70e9b-2613-45f7-be0b-5f6692786149)

```python
plt.imshow(X[0].reshape(28,28), cmap="grey")
```
![image](https://github.com/user-attachments/assets/8090865c-e0c8-42b7-a0de-4679b504475d)


`-` step2 : 학습가능한 오브젝트들의 설정 (모델링 과정 포함)

```python
torch.manual_seed(1)
net = torch.nn.Sequential(
    torch.nn.Linear(784,32),
    torch.nn.ReLU(), # n,32차원
    torch.nn.Linear(32,1), # 1차원으로
    torch.nn.Sigmoid() # 0~1사이의 숫자로 만들어줌
)
loss_fn = torch.nn.BCELoss()
optimizr = torch.optim.SGD(net.parameters()) # 원래 아담이었는데 너무 빨리 맞춰서 일부러 안좋은 것 사용
```

`-` step3 : 학습 (=적합)

```python
for epoc in range(1,501):
    #----에폭시작-----#
    # step1
    yhat = net(X) # 최초의 직선
    # step2
    loss = loss_fn(yhat,y) # 로스구함
    # step3
    loss.backward() # 로스 미분
    # step4
    optimizr.step() # 파라미터 업데이트
    optimizr.zero_grad() # 파라미터 초기화
    #-----에폭끝-----#
    # 에폭별로 살펴보고 싶은 뭔가들..
    if (epoc % 50) == 0: # 에폭이 50씩 증가할때 마다 실행됌
        acc = ((net(X).data > 0.5) == y).float().mean()
        print(f"# of epochs = {epoc},\t acc={acc.item(): .2f}")
```
![image](https://github.com/user-attachments/assets/ea5cf526-7197-409a-b894-67d591619542)


`-` step 4 : 예측 and 결과분석

- train acc

```python
((net(X) > 0.5)*1.0 ==  y).float().mean()
```
- 0.9936

- test  acc

```python
((net(XX) > 0.5)*1.0 ==  yy).float().mean()
```
- 0.9986

## B. dropout 사용

- step1 : 데이터 정리
- step2 : 학습가능한 오브젝트들의 설정 (모델링과정 포함)

```python
torch.manual_seed(1)
net = torch.nn.Sequential(
    torch.nn.Linear(784,32),
    torch.nn.Dropout(0.9),
    torch.nn.ReLU(),
    torch.nn.Linear(32,1),
    torch.nn.Sigmoid()
)
loss_fn = torch.nn.BCELoss()
optimizr = torch.optim.SGD(net.parameters())
```

- step3 : 학습(=적합)

```python
for epoc in range(1,501):
    net.train()
    #----에폭시작-----#
    # step1
    yhat = net(X)
    # step2
    loss = loss_fn(yhat,y)
    # step3
    loss.backward()
    # step4
    optimizr.step()
    optimizr.zero_grad()
    #-----에폭끝-----#
    net.eval()
    # 에폭별로 살펴보고 싶은 뭔가들..
    if (epoc % 50) == 0:
        acc = ((net(X).data > 0.5) == y).float().mean()
        print(f"# of epochs = {epoc},\t acc={acc.item(): .2f}")
```

![image](https://github.com/user-attachments/assets/ebd4d7d9-5d5d-4e75-89bf-29849c3d7fb0)


`-` step 4 : 예측 and 결과분석

- train acc

```python
((net(X) > 0.5)*1.0 ==  y).float().mean()
```
- 0.9927

- test  acc

```python
((net(XX) > 0.5)*1.0 ==  yy).float().mean()
```
- 오류발생함
- 방법 1 : net을 cpu로 내림
- 방법 2 :net을 cuda에 유지 ( = XX, YY를 cuda로 올림)

```python
# net를 쿠다에 유지하는 방법으로 해보자..
XX = XX.to("cuda:0")
yy = yy.to("cuda:0")
((net(XX) > 0.5) ==  yy).float().mean()
```

- tensor(0.9991, device='cuda:0')

## D. 미니배치도 사용

- step1 : 데이터 정리

```python
X = X.to("cpu")
y = y.to("cpu")
XX = XX.to("cpu")
yy = yy.to("cpu")

ds = torch.utils.data.TensorDataset(X,y)
dl = torch.utils.data.DataLoader(ds, batch_size=16)
```

- step2 : 학습가능한 오브젝트들의 설정(모델링과정 포함)

```python
torch.manual_seed(1)
net = torch.nn.Sequential(
    torch.nn.Linear(784,32),
    torch.nn.Dropout(0.5),
    torch.nn.ReLU(),
    torch.nn.Linear(32,1),
    torch.nn.Sigmoid()
).to("cuda:0")
loss_fn = torch.nn.BCELoss()
optimizr = torch.optim.SGD(net.parameters())
```

- step3 : 학습(=적합)

```python
for epoc in range(1,3):
    net.train()
    #----에폭시작-----#
    for Xm,ym in dl:
        Xm = Xm.to("cuda:0")
        ym = ym.to("cuda:0")
        # step1
        ym_hat = net(Xm)
        # step2
        loss = loss_fn(ym_hat,ym)
        # step3
        loss.backward()
        # step4
        optimizr.step()
        optimizr.zero_grad()
    #-----에폭끝-----#
    net.eval()
    # 에폭별로 살펴보고 싶은 뭔가들..
        # ## 방법1 -- net를 cpu로 내림
        # net.to("cpu")
        # acc = ((net(X.data) > 0.5) == y.data).float().mean()
        # print(f"# of epochs = {epoc},\t acc={acc.item(): .4f}")
        # net.to("cuda:0")
    ## 방법2 -- net을 cuda에 유지
    s = 0
    for Xm,ym in dl:
        Xm = Xm.to("cuda:0")
        ym = ym.to("cuda:0")
        s = s + ((net(Xm).data > 0.5) == ym.data).float().sum()
    acc = s/12665
    print(f"# of epochs = {epoc},\t acc={acc.item(): .4f}")
```

![image](https://github.com/user-attachments/assets/cb275d28-3a1b-4048-a181-e9a86c2acc8d)


- step4 : 예측 and 결과분석

```python
net.to("cpu")
```

- train acc

```python
((net(X) > 0.5)*1.0 ==  y).float().mean()
```

- test acc

```python
((net(XX) > 0.5)*1.0 ==  yy).float().mean()
```

> 코드가 점점 더러워짐 -> trainer의 개념 등장

# 2. 다항분류

## A. 이항분류와 `BCEWithLogitsLoss`

`-` 데이터 

```python
train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)
# test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True)
to_tensor = torchvision.transforms.ToTensor()
X0_train = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==0])
X1_train = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==1])
# X0_test = torch.stack([to_tensor(Xi) for Xi, yi in test_dataset if yi==0])
# X1_test = torch.stack([to_tensor(Xi) for Xi, yi in test_dataset if yi==1])
X = torch.concat([X0_train,X1_train],axis=0).reshape(-1,784)
y = torch.tensor([0.0]*len(X0_train) + [1.0]*len(X1_train)).reshape(-1,1)
# XX = torch.concat([X0_test,X1_test],axis=0).reshape(-1,784)
# yy = torch.tensor([0.0]*len(X0_test) + [1.0]*len(X1_test)).reshape(-1,1)
```

- 예전코드는 아래와 같다.(sig는 수동처리함)

```python
torch.manual_seed(0)
net = torch.nn.Sequential(
    torch.nn.Linear(784,32),
    torch.nn.ReLU(),
    torch.nn.Linear(32,1)
)
loss_fn = torch.nn.BCELoss()
optimizr = torch.optim.Adam(net.parameters())
#---#
for epoc in range(1,11):
    # step1
    netout = net(X) # netout = logits
    yhat = torch.exp(netout) / (1 + torch.exp(netout)) # yhat = prob
    # step2
    loss = loss_fn(yhat,y)
    # step3
    loss.backward()
    # step4
    optimizr.step()
    optimizr.zero_grad()
    #---에폭끝나고 확인할 것들---#
    acc = ((net(X).data > 0)  == y).float().mean()
    print(f"epoch = {epoc}\t acc = {acc:.4f}")
```
![image](https://github.com/user-attachments/assets/86796511-7e7c-40d4-a40a-ede4e7e860ba)

`#` netout(= logits) 의 특징

- $netout > 0 \Leftrightarrow sig(netout) >0.5$
- $netout < 0 \Leftrightarrow sig(netout) <0.5$

- 그런데 위의 코드는 아래의 코드와 같음

```python
torch.manual_seed(0)
net = torch.nn.Sequential(
    torch.nn.Linear(784,32),
    torch.nn.ReLU(),
    torch.nn.Linear(32,1)
)
loss_fn = torch.nn.BCEWithLogitsLoss() # <--- 여기를 바꾸고
optimizr = torch.optim.Adam(net.parameters())
#---#
for epoc in range(1,11):
    # step1
    netout = net(X) # netout = logits
    # yhat = torch.exp(netout) / (1 + torch.exp(netout))  # yhat = prob
    # step2
    loss = loss_fn(netout,y)
    # step3
    loss.backward()
    # step4
    optimizr.step()
    optimizr.zero_grad()
    #---에폭끝나고 확인할 것들---#
    acc = ((net(X).data > 0)  == y).float().mean()
    print(f"epoch = {epoc}\t acc = {acc:.4f}")
```
![image](https://github.com/user-attachments/assets/5002b9ec-1f2a-40cc-be3a-94e316167a3e)

## B. 범주형자료의 변환

`-` 범주형자료를 숫자로 어떻게 바꿀까?

- 실패 / 성공 $\to$ 0 / 1
- 숫자0그림 / 숫자1그림 $\to$ 0 / 1
- 강아지그림 / 고양이그림 $\to$ 0 / 1
- 강아지그림 / 고양이그림 / 토끼그림 $\to$ 0 / 1 / 2 ?????

`-` 주입식교육: 강아지그림/고양이그림/토끼그림일 경우 숫자화시키는 방법

- 잘못된방식: 강아지그림 = 0, 고양이그림 = 1, 토끼그림 = 2
- 올바른방식: 강아지그림 = [1,0,0], 고양이그림 = [0,1,0], 토끼그림 = [0,0,1]  ### <-- 이런방식을 원핫인코딩이라함

`-` 왜?

- 설명1: 강아지그림, 고양이그림,  토끼그림은 서열측도가 아니라 명목척도임. 그래서 범주를 0,1,2 로 숫자화하면 평균등의 의미가 없음 (사회조사분석사 2급 스타일)
- 설명2: 범주형은 원핫인코딩으로 해야함 ("30일만에 끝내는 실전머신러닝" 이런 책에 나오는 스타일)
- 설명3: 동전을 한번 던져서 나오는 결과는 $n=1$인 이항분포를 따름. 주사위 한번 던져서 나오는 눈금의 숫자는 $n=1$인 다항분포를 따름. $n=1$인 이항분포의 실현값은 0,1 이고, $n=1$인 다항분포의 실현값은 [1,0,0], [0,1,0], [0,0,1] 이므로 당연히 $y_i$ 는 [1,0,0], [0,1,0], [0,0,1] 중 하나의 형태를 가진다고 가정하는게 바람직함 (이 설명이 이 중에서 가장 정확한 설명임)

## C. 실습 : 3개의 클래스를 구분

`-` 데이터준비

```python
train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)
to_tensor = torchvision.transforms.ToTensor()
X0 = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==0])
X1 = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==1])
X2 = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==2])
X = torch.concat([X0,X1,X2]).reshape(-1,1*28*28)
y = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2)).reshape(-1,1).float()
```

```python
y = torch.nn.functional.one_hot(y.flatten().long()).float()
```

`-` 적합

```python
torch.manual_seed(43052)
net = torch.nn.Sequential(
    torch.nn.Linear(784,32),
    torch.nn.ReLU(),
    torch.nn.Linear(32,3),
)
loss_fn = torch.nn.CrossEntropyLoss() # 이름이 좀 그래.. 나같으면 CEWithLogitsLoss 라고 했을듯
optimizr = torch.optim.Adam(net.parameters())
#---#
for epoc in range(100):
    ## step1
    netout = net(X)
    ## step2
    loss = loss_fn(netout,y)
    ## step3
    loss.backward()
    ## step4
    optimizr.step()
    optimizr.zero_grad()
```

```python
(net(X).argmax(axis=1)  == y.argmax(axis=1)).float().mean()
```
- 0.9827


![image](https://github.com/user-attachments/assets/95c6f125-850c-4065-b765-12a383f73d75)
























