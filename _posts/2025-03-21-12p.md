
---

layout: single
title: "[CD] 인공지능 뉴스 기사 빈도수 분석" 
categories: [CD]
tags : [CD]

---




# 인공지능 뉴스 기사 분석: PPDAC 사이클을 활용한 Python 웹 스크래핑 실전

## PPDAC 사이클 개요
**PPDAC**은 데이터 분석의 다섯 단계 프레임워크입니다:  
> **P**roblem → **P**lan → **D**ata → **A**nalysis → **C**onclusion

이번 프로젝트에서는 **Python을 사용해 ‘인공지능’ 관련 네이버 뉴스 기사를 웹 스크래핑**하고, **언론사별 기사 수를 분석**해보겠습니다.

---

## 1. Problem (문제 정의)

### 핵심 질문:
> “인공지능(AI)”이라는 주제는 어떤 언론사에서 더 많이 다루고 있을까?

- 최근 ChatGPT, 생성형 AI 등으로 인해 **AI 관련 뉴스가 급증**
- 어떤 **언론사가 이 주제에 집중하는지**를 파악하면,
  - 기술 뉴스 트렌드 이해
  - 언론사별 보도 성향 분석에 활용 가능

---

## 2. Plan (계획 수립)

### 목표
- 네이버 뉴스에서 “인공지능” 키워드로 **다수의 뉴스 기사 제목, 링크, 언론사**를 수집
- 언론사별 기사 수를 **시각화 및 통계적 해석**

### 🛠 사용 도구
- `requests`, `BeautifulSoup` → 웹 크롤링
- `pandas` → 데이터 처리
- `matplotlib` → 시각화
- `urllib.parse.quote()` → 한글 URL 인코딩

---

## 3. Data (데이터 수집)

###  여러 페이지에서 뉴스 기사 수집 코드 (최대 50개 기사)

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd
from urllib.parse import quote
import time

keyword = quote("인공지능")
num_pages = 5  # 총 50개 기사 (10개 * 5페이지)

titles, links, media = [], [], []
headers = {"User-Agent": "Mozilla/5.0"}

for page in range(num_pages):
    start = page * 10 + 1
    url = f"https://search.naver.com/search.naver?where=news&query={keyword}&start={start}"
    res = requests.get(url, headers=headers)
    soup = BeautifulSoup(res.text, "html.parser")

    news_items = soup.select("a.news_tit")
    press_elements = soup.select(".info_group a.press")

    titles += [item.get_text() for item in news_items]
    links += [item["href"] for item in news_items]
    media += [item.get_text() for item in press_elements]

    time.sleep(1)  # 서버 과부하 방지

# 데이터프레임 생성
df = pd.DataFrame({
    "Title": titles,
    "Link": links,
    "Media": media
})
```

---

## 4. Analysis (데이터 분석 및 시각화)

### ▶ 언론사별 기사 수 집계

```python
media_counts = df["Media"].value_counts().reset_index()
media_counts.columns = ["Media", "Count"]
```

### ▶ 시각화

```python
import matplotlib.pyplot as plt

plt.rcParams["font.family"] = "Malgun Gothic"  # 한글 폰트
plt.figure(figsize=(14, 6))
plt.bar(media_counts["Media"], media_counts["Count"])
plt.xticks(rotation=45)
plt.title("언론사별 '인공지능' 관련 기사 수")
plt.xlabel("언론사")
plt.ylabel("기사 수")
plt.tight_layout()
plt.show()
```

![image](https://github.com/user-attachments/assets/b2591088-5823-4ff6-b80f-9e4a5065ca7f)

---

### 통계적 해석

- 전체 기사 중 **상위 5개 언론사가 약 절반 이상**을 차지 → **편중된 분포**
- 많은 언론사는 **1건의 기사만 보도** → 긴 꼬리(long tail) 구조
- 이는 **파레토 분포(80/20 법칙)**와 유사: 소수 언론사가 대다수 기사 생산
- 평균 기사 수보다 높은 언론사들은 **AI에 높은 관심 or 전문성** 보유

---

## 5.  Conclusion (결론 도출)

### 🔹 분석 결과 요약

| 항목 | 내용 |
|------|------|
| 분석 키워드 | 인공지능 |
| 수집 기사 수 | 50개 (5페이지 기준) |
| 언론사 수 | 약 25개 |
| 주요 보도 언론사 | 연합뉴스TV, 뉴시스언론사 선정 |
| 분포 특성 | 상위 편중, 비대칭 분포, 롱테일 구조 |

---

### 🔄 확장 가능성

- **키워드 다양화**: "AI", "오픈AI", "챗GPT" 등과 비교
- **감성 분석**: 제목 기반 긍/부정 분석
- **시간 흐름 분석**: 날짜별 기사 수 집계로 트렌드 시각화
- **자동화**: 특정 키워드를 정기적으로 수집해서 데이터 축적

---

## ✅ 요약 한 줄

> “인공지능” 뉴스 보도는 일부 언론사에 집중되어 있으며, 언론사별 보도 편차가 크다는 점에서 정보 접근의 불균형을 보여준다.
