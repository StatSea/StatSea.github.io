---
layout: single
title: "[DL] 딥러닝 03wk"
categories: [DL]
tags: [DL]
mathjax: true
---
딥러닝 03wk 이해하기


---
# 기본 세팅
```python
import torch
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (4.5, 3.0)
```

```python
torch.manual_seed(43052)
x,_ = torch.randn(100).sort()
eps = torch.randn(100)*0.5
X = torch.stack([torch.ones(100),x],axis=1)
W = torch.tensor([[2.5],[4.0]])
y = X@W + eps.reshape(100,1)
x = X[:,[1]]
```
---
# 파이토치식 코딩 패턴
## A. bias의 사용
- net 에서 bias 를 사용

```python
# step1을 위한 사전준비 -> net 설정
net = torch.nn.Linear(
    in_features=1,
    out_features=1,
    bias=True
) # net(x) = x@net.weight.T + net.bias
net.bias.data = torch.tensor([-5.0])
net.weight.data = torch.tensor([[10.0]])
```

```python
# step2를 위한 사전준비 -> loss 설정
loss_fn = torch.nn.MSELoss()
```

```python
# step4를 위한 사전준비 -> optimizr 설정
optimizr = torch.optim.SGD(net.parameters(),lr=0.1)
```

```python
# 에폭 설정
for epoc in range(30):
    # step1: yhat
    yhat = net(x)
    # step2: loss
    loss = loss_fn(yhat,y)
    # step3: 미분
    loss.backward()
    # step4: update
    optimizr.step()
    optimizr.zero_grad()
```
---
### 전체 코드

```python

# step1을 위한 사전준비
net = torch.nn.Linear(
    in_features=1,
    out_features=1,
    bias=True
) # net(x) = x@net.weight.T + net.bias
net.bias.data = torch.tensor([-5.0])
net.weight.data = torch.tensor([[10.0]])
# step2를 위한 사전준비
loss_fn = torch.nn.MSELoss()
# step4를 위한 사전준비
optimizr = torch.optim.SGD(net.parameters(),lr=0.1)
for epoc in range(30):
    # step1: yhat
    yhat = net(x)
    # step2: loss
    loss = loss_fn(yhat,y)
    # step3: 미분
    loss.backward()
    # step4: update
    optimizr.step()
    optimizr.zero_grad()

```

```python
net.bias.data, net.weight.data
```
- (tensor([2.4290]), tensor([[4.0144]]))가 출력된다.
---
### 잘못된 코드?
```python
# step1을 위한 사전준비
net = torch.nn.Linear(
    in_features=2,
    out_features=1,
)
net.weight.data = torch.tensor([[-5.0,  10.0]])
# step2를 위한 사전준비
loss_fn = torch.nn.MSELoss()
# step4를 위한 사전준비
optimizr = torch.optim.SGD(net.parameters(),lr=0.1)
for epoc in range(30):
    # step1: yhat
    yhat = net(X)
    # step2: loss
    loss = loss_fn(yhat,y)
    # step3: 미분
    loss.backward()
    # step4: update
    optimizr.step()
    optimizr.zero_grad()
```

- 잘못된 이유가 무엇일까?

```python
# step1을 위한 사전준비
net = torch.nn.Linear(
    in_features=2,
    out_features=1,
)
yhat = net(X) = X@net.weight.T + net.bias
net.weight
```
``` python
Parameter containing:
tensor([[-1.0241,  4.0080]], requires_grad=True)
```
- 위와 같은 결과가 출력된다.
- 입력차원이 2차원이어야 가능하므로 차원이 맞지 않으면 에러가 나는 상황이 발생한다.
---
# 로지스틱 모형
---
## A. $$\hat{\bf y} = ??$$
- x를 가지고 y를 맞추는 문제
```python
x = torch.tensor([-6,-5,-4,-3,-2,-1, 0, 1, 2, 3, 4, 5, 6.0]).reshape(-1,1)
y = torch.tensor([ 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1]).reshape(-1,1)
```

- 아래와 같이 모형화 한다면?
```python
plt.plot(x,y,'o', label=r"observed data (with error) = $(x_i,y_i)$")
plt.plot(x,torch.exp(x)/(1+torch.exp(x)),'o--', label = "underlying (without error)")
plt.legend()
```

![image](https://github.com/user-attachments/assets/b5c1813d-3fe3-42e3-8700-655d14a91ffb)

---

## B. $$\hat{\bf y} = \frac{\exp(\text{linr}({\bf X}))}{1+\exp(\text{linr}({\bf X}))}$$
- 산점도가 꼭 이런 경우가 아닌경우
![image](https://github.com/user-attachments/assets/91fcc6ad-b952-4d23-955c-eee78f5da0a5)

- x가 증가할수록 y가 0이 된다면?
- 0근처에서 변화가 일어나지 않고 2근처에서 변화가 일어난다면?
- 변화가 더 급하거나 완만하게 일어난다면?

```python
plt.plot(x,y,'o', label=r"observed data (with error) = $(x_i,y_i)$")
plt.plot(x,torch.exp(5*x+3)/(1+torch.exp(5*x+3)),'o--', label = "underlying (without error)")
plt.legend()
```
![image](https://github.com/user-attachments/assets/b86ee8da-a2b7-4021-9eac-63aa695f8f1c)

- 걱정해결
```python
plt.plot(x,torch.exp(x)/(1+torch.exp(x)),'o--', label = "underlying type1 (without error)", color="C1")
plt.plot(x,torch.exp(5*x)/(1+torch.exp(5*x)),'o--', label = "underlying type2 (without error)", color="C2")
plt.legend()
```
- 안에 있는 경사도를 다르게 해서 형태가 달라보인다.
![image](https://github.com/user-attachments/assets/710925f0-08cc-40df-97a6-a5f6c09df4ad)

---
### 회귀 vs 로지스틱
- $${\bf X} \to {\bf y}$$ 에 대한 패턴이 $$\text{linr}({\bf X}) \approx {\bf y}$$ 이라면 회귀!
- $${\bf X} \to {\bf y}$$ 에 대한 패턴이 $$\frac{\exp(\text{linr}({\bf X}))}{1+\exp(\text{linr}({\bf X}))} \approx {\bf y}$$ 이라면 로지스틱!
---
## C. 로지스틱 모형
- x가 커지거나 작아질수록 y=1이 잘나오는 모형은 아래와 같이 설계할 수 있다.

  -   $$y_i \sim {\cal B}(\pi_i),\quad$$ where
      $$\pi_i = \frac{\exp(w_0+w_1x_i)}{1+\exp(w_0+w_1x_i)} = \frac{1}{1+\exp(-w_0-w_1x_i)}$$
  
  -   $$\hat{y}_i= \frac{\exp(\hat{w}_0+\hat{w}_1x_i)}{1+\exp(\hat{w}_0+\hat{w}_1x_i)}=\frac{1}{1+\exp(-\hat{w}_0-\hat{w}_1x_i)}$$

- 회귀모형과 로지스틱 모형의 비교
  - 회귀모형 : $$y_i \sim {\cal N}(w_0+w_1x_i, \sigma^2)$$
  - 로지스틱 : $$y_i \sim {\cal B}\big(\frac{\exp(w_0+w_1x_i)}{1+\exp(w_0+w_1x_i)}\big)$$

- 우리가 예측하려는 것
  - 회귀모형 : 정규분포의 평균
    - 예측값 : $$\hat{w}_0 + \hat{w}_1x_i$$
  - 로지스틱 : 베르누이의 평균
    - 예측값 : $$\frac{\exp(\hat{w}_0+\hat{w}_1x_i)}{1+\exp(\hat{w}_0+\hat{w}_1x_i)}$$


