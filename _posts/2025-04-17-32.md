---
layout: single
title: "[ML] 기계학습 Chap 4(2) "
categories: [ML]
tags: [ML]
mathjax: true
---
기계학습 Chap 4 정리2

---

# 예시 ) default dataset

- lda 적용 결과 10000개의 훈련자료 중 2.75%만이 오분류 되었다.
- training error rate = 2.75%
- 이 훈련자료의 3.33%만이 채무 불이행 (default = yes) 이므로, 모든 관측치를 defaul=no 로 분류하는 분류기를 사용해도 오직 3.33%의 오류만 발생한다.
- 전체적인 오분류율 만이 아닌 좀 더 특수한 상황에서의 평가지표들이 필요해 보인다.

# 혼동 행렬 Confusion Matrix

![image](https://github.com/user-attachments/assets/8eacf1c8-9fe1-4ca5-8b16-d122de5ef1b5)

- 실제 채무불이행자들에 대해서는 75%가 넘는 오분류가 나타났다.
- 손해를 끼칠 위험이 큰 고객들을 분별해 내야 하는 회사 입장에서는 위와 같은 결과가 바람직하지 않다.
- 베이즈 분류기에서도 전체 오류율을 최소화시키려 하기 때문에 특정 범주에 대해서는 좋은 성능을 담보하지 않을 수 있다.

![image](https://github.com/user-attachments/assets/f8503997-c10a-467d-a77f-e1b57554ed41)


# 민감도 (sensitivity) , 특이도 (specificity)

- 범주가 0과 1로 분류된다고 할 때 두 측도는 아래와 같이 정의된다.

`-` 민감도 : 1을 1로 분류하는 비율

`-` 특이도 : 0을 0으로 분류하는 비율

- default dataset의 경우, 민감도는 24.3% (81/333) , 특이도는 99.8% (9644/9667)로 민감도가 매우 떨어지는 것을 알 수 있음
- 81 , 9644개만 0.5의 확률을 넘고 1과 0이 된 것

- 임계치 조정 : 범주가 두 개인 경우 베이즈 분류기는 다음과 같은 조건을 만족하는 관측치에 한하여 default = yes로 분류

$$
Pr(default = YES \mid X = x) > 0.5
$$

- 만약 임계치를 0.2로 바꾼다면 민감도와 특이도가 변하게 된다.
- 민감도와 특이도는 서로 반비례 관계

# 임계치 조정
![image](https://github.com/user-attachments/assets/f89be9b3-612a-4456-8b13-b290d8085dca)

- 임계치를 높일 경우 : 에러율 증가, 특이도 증가, 민감도 하락
  - 대부분 음성으로 판단하기 때문
- 임계치를 낮출 경우 : 에러율 하락, 특이도 하락, 민감도 증가
  - 대부분 양성으로 판단하기 때문

- 따라서 임계치의 설정에는 도메인 지식이 중요하다.

# LOC 커브
- 민감도와 특이도를 동시에 타나내어 분류기의 성능을 평가하는 대표적인 방법
- 임계치를 변화시키면서 2차원 좌표평면 상에 나타낸 곡선
- 이상적으로는 왼쪽 상단을 통과하는 것이 좋다.
  - 이 경우 AUC 가 1이 된다.
- 곡선 면적이 1에 가까울수록 분류기의 성능이 좋다.
- 곡선 아래 면적 : AUC

![image](https://github.com/user-attachments/assets/ed345feb-53b2-4704-8302-36e7134c17b4)
![image](https://github.com/user-attachments/assets/945ead44-7047-4109-86bd-c89d2e0244c6)


# QDA 

$$
\text{각 범주를 특정하는 정규분포의 분산에 이질성을 허용함. 즉, }  
X \sim \mathcal{N}(\mu_k, \Sigma_k)
$$

$$
\text{이 경우 판별함수는 다음과 같은 형태가 됨이 알려져 있다.}
$$

$$
\delta_k(x) = -\frac{1}{2}(x - \mu_k)^\top \Sigma_k^{-1}(x - \mu_k)
- \frac{1}{2} \log |\Sigma_k| + \log \pi_k
$$

$$
\text{판별함수가 } x \text{에 대한 2차식의 형태로 주어짐.}
$$

$$
\text{LDA에 비해 유연한(복잡한) 모형임.}
$$

- 판별 함수에 x가 두번 등장하는 것이 특징이다.
- 그래프도 선형이 아닌 곡선으로 나온다.

![image](https://github.com/user-attachments/assets/43b91158-9301-401c-86bc-2d83c06f10f6)


# Naive Bayes 
$$
\text{Bayes 규칙에 따라 주어진 예측변수들이 } x_1, \dots, x_p \text{이고,  
분류의 대상 변수가 } A \in \{0, 1\} \text{라고 할 때,}
$$

$$
Pr(A = a \mid x_1, \dots, x_p) = 
\frac{Pr(x_1, \dots, x_p \mid A = a) \cdot Pr(A = a)}
{Pr(x_1, \dots, x_p)}
$$

---

- 여기에서 \( Pr(x_1, \dots, x_p \mid A = a) \)는  
  \( p \)가 클 경우 계산이 어려움

- 이를 해결하기 위해 예측변수들이 **조건부 독립**이라는 가정을 하여,  
  아래와 같이 확률을 분해하면 이것을 **Naive Bayes 분류기**라고 함:

$$
Pr(x_1, \dots, x_p \mid A = a) = 
\prod_{i=1}^{p} Pr(x_i \mid A = a)
$$

- 데이터가 100 x 100 이면 분산이 10000개가 필요하다..

# K-nearest neighbors
$$
\text{조건부 확률을 인접한 } K \text{개의 데이터 포인트의 상대비율로 추정함.}
$$

$$
Pr(Y = j \mid X = x_0) = \frac{1}{K} \sum_{i \in \mathcal{N}_0} I(y_i = j)
$$

$$
\mathcal{N}_0 : x_0 \text{와 가장 가까운 } K \text{개의 자료의 집합임.  
그리고 거리를 계산할 때, } y \text{는 사용하지 않음에 유의.}
$$

---

$$
\text{위 확률을 최대화하는 } j \text{로 관측치를 분류함.}
$$

$$
K \text{의 선택이 분류기의 성능을 결정하는 데 매우 핵심적인 역할을 함.}
$$

![image](https://github.com/user-attachments/assets/7ca3e789-bb0f-4e12-be6c-6d8c99baa3ad)

# 분류기의 성능 비교
- 범주가 2개일 때, LDA 와 로지스틱 모형은 선형적인 decision boundary를 생성한다는 측면에서 유사
- 각 범주의 분포가 정규분포로 잘 근사되는지 여부에 의해서 두 방식의 성능이 엇갈릴 수 있음
- KNN은 decision boundary에 대해 어떠한 가정도 하지 않아서 decision boundary가 비선형인 경우 두 방식에 비해 우월성을 보일 수 있음
- QDA는 qudratic decision boundary를 설정한다는 면에서 KNN과 LDA 혹은 로지스틱의 중간 쯤에 위치하는 방법이다.
- 모형의 복잡도 (유연성) 은 LDA ~ 로지스틱 < QDA < KNN이다.
- 예측변수, 반응변수 범주 각각 2개
  - S1: 각 범주내의 변수들은 서로 독립인 정규분포.
  - S2: 각 범주내 변수들의 상관계수가 -0.5. 다른 조건은 S1과 동일.
  - S3: 각 범주내의 변수들은 𝑡분포.
  - S4: 각 범주내의 변수들은 각각 상관계수가 0.5, -0.5인 정규분포.
  - S5: 각 범주내의 변수들은 서로 독립인 정규분포. 반응변수가 두 변수의 이차
  다항식을 이용한 결합으로 생성됨.
  - S6: 각 범주내의 변수들은 서로 독립인 정규분포. 반응변수가 두 변수의 복잡한
  비선형결합으로 생성됨.
- 5개의 분류기
  - KNN, 𝐾 = 1 / KNN, 𝐾는 CV로 결정 / LDA / 로지스틱 모형 / QDA
 
# 예시

- KNN-1: 최근접 이웃 1개 사용

- KNN-CV: 교차검증으로 선택된 K 사용

- LDA: 선형 판별 분석

- Logistic: 로지스틱 회귀

- QDA: 이차 판별 분석

![image](https://github.com/user-attachments/assets/b2cb79b0-7bda-4215-918d-4fde103ca9f4)
![image](https://github.com/user-attachments/assets/63fe08dc-0de1-4714-8c2d-ec4785476b96)
![image](https://github.com/user-attachments/assets/6f4a6977-c0c3-461f-be69-10cdf1167cf7)

- QDA: 데이터가 충분하고 클래스 간 분산이 다를 때 강력하지만, 과적합 위험 있음

- LDA/Logistic: 단순한 결정 경계에서 매우 안정적

- KNN-1: 민감하고 노이즈에 취약함

- KNN-CV: 일반적으로 가장 균형 잡힌 성능

## 분류기의 성능 비교
- 모든 상황에서 우월한 분류기는 없다.
- 간단한 상황 = LDA , 로지스틱 모형
- 복잡한 상황 = KNN , QDA
- KNN에서도 적절한 수의 K를 설정하는 것이 필요
