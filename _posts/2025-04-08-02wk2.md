---
layout: single
title: "[DL] 딥러닝 02wk2"
categories: [DL]
tags: [DL]
mathjax: true
---

파라미터 학습과정 시각화

---
## 기본세팅

```python
import torch
import numpy as np
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (4.5, 3.0)
```

```python
torch.manual_seed(43052)
x,_ = torch.randn(100).sort()
eps = torch.randn(100)*0.5
X = torch.stack([torch.ones(100),x],axis=1)
W = torch.tensor([[2.5],[4.0]])
y = X@W + eps.reshape(100,1)
x = X[:,[1]]
```

## 1. 파라미터 학습과정

```python
What = torch.tensor([[-5.0],[10.0]],requires_grad=True)
alpha = 0.001
print(f"시작값 = {What.data.reshape(-1)}")
for epoc in range(30):
    yhat = X @ What
    loss = torch.sum((y-yhat)**2)
    loss.backward()
    What.data = What.data - alpha * What.grad
    print(f'loss = {loss:.2f} \t 업데이트폭 = {-alpha * What.grad.reshape(-1)} \t 업데이트결과: {What.data.reshape(-1)}')
    What.grad = None
```

- 이 코드를 돌려보면 epoc 수치에 따라 업데이트가 얼마나 되는지 알 수 있다.

## 2. 시각화 -- yhat 관점

- 원래 함수 그리기
```python
What = torch.tensor([[-5.0],[10.0]],requires_grad=True)
alpha = 0.001
plt.plot(x,y,'o',label = "observed")
```

- 최초의 직선 하나 그리기
```python
fig = plt.gcf()
ax = fig.gca()
ax.plot(x,X@What.data,'--',color="C1")
```

- epoc을 늘려가면서 직선 그리기
```python
for epoc in range(30):
    yhat = X @ What
    loss = torch.sum((y-yhat)**2)
    loss.backward()
    What.data = What.data - alpha * What.grad
    ax.plot(x,X@What.data,'--',color="C1",alpha=0.1)
    What.grad = None
```


## 3. 시각화 -- loss의 관점

- 손실 함수를 3D 그래프로 시각화하는 함수 생성
---

- 3d 그래프를 그릴 수 있게 설정

```python
fig = plt.figure()
ax = fig.add_subplot(projection='3d')
```

- w0과 w1을 각각 -6부터 10까지 0.5 간격으로 생성
- np.meshgrid는 2차원 격자 형태의 좌표 데이터를 생성
```python
w0 = np.arange(-6, 11, 0.5)
w1 = np.arange(-6, 11, 0.5)
W1, W0 = np.meshgrid(w1, w0)
```

- LOSS는 손실 값을 저장할 배열이며, W0와 같은 크기로 초기화
```python
LOSS = W0 * 0
```

- 각각의 w0[i]와 w1[j] 조합에 대해 손실값(MSE, Mean Squared Error)을 계산
- y - w0[i] - w1[j]*x는 예측값과 실제값의 차이

```python
for i in range(len(w0)):
    for j in range(len(w1)):
        LOSS[i,j] = torch.sum((y - w0[i] - w1[j] * x) ** 2)
```

- 위에서 계산된 LOSS를 바탕으로 3D 표면(surface) 그래프를 생성

```python
ax.plot_surface(W0, W1, LOSS, rstride=1, cstride=1, color='b', alpha=0.1)
```

- 시점, 축, 틱 설정
- 자동으로 그림이 출력되지 않게 하고  fig 객체 반환
```python
ax.azim = 30
ax.dist = 8
ax.elev = 5
ax.set_xlabel(r'$w_0$')
ax.set_ylabel(r'$w_1$')
ax.set_xticks([-5, 0, 5, 10])
ax.set_yticks([-5, 0, 5, 10])
plt.close(fig)
return fig
```

### 정리한 코드
```python
def plot_loss():
    fig = plt.figure()
    ax = fig.add_subplot(projection='3d')
    w0 = np.arange(-6, 11, 0.5)
    w1 = np.arange(-6, 11, 0.5)
    W1,W0 = np.meshgrid(w1,w0)
    LOSS=W0*0
    for i in range(len(w0)):
        for j in range(len(w1)):
            LOSS[i,j]=torch.sum((y-w0[i]-w1[j]*x)**2)
    ax.plot_surface(W0, W1, LOSS, rstride=1, cstride=1, color='b',alpha=0.1)
    ax.azim = 30  ## 3d plot의 view 조절
    ax.dist = 8   ## 3d plot의 view 조절
    ax.elev = 5   ## 3d plot의 view 조절
    ax.set_xlabel(r'$w_0$')  # x축 레이블 설정
    ax.set_ylabel(r'$w_1$')  # y축 레이블 설정
    ax.set_xticks([-5,0,5,10])  # x축 틱 간격 설정
    ax.set_yticks([-5,0,5,10])  # y축 틱 간격 설정
    plt.close(fig)  # 자동 출력 방지
    return fig
```

### 시각화 해본다면?

- loss 함수
```python
def l(w0hat,w1hat):
    yhat = w0hat + w1hat*x
    return torch.sum((y-yhat)**2)
```

- 시각화
```python
fig = plot_loss()
ax = fig.gca()
# 포인트 시각화
ax.scatter(2.5, 4, l(2.5,4), s=200, marker='*', color='red', label=r"${\bf W}=[2.5, 4]'$")
ax.scatter(-5, 10, l(-5,10), s=200, marker='*', color='blue', label=r"initial $\hat{\bf W}=[-5, 10]'$")
ax.legend()
fig
```

- epoc 증가할때 시각화
```python
What = torch.tensor([[-5.0],[10.0]],requires_grad=True)
alpha = 0.001
for epoc in range(30):
    yhat = X @ What
    loss = torch.sum((y-yhat)**2)
    loss.backward()
    What.data = What.data - 0.001 * What.grad
    w0,w1 = What.data.reshape(-1) # reshape 해주기
    ax.scatter(w0,w1,l(w0,w1),s=5,marker='o',color='blue') # blue색 점을 그래프에 찍기
    What.grad = None # 초기화
```
![image](https://github.com/user-attachments/assets/48384698-9f84-49eb-be15-e6ab527f28bd)

## 4. 애니메이션
```python
from matplotlib import animation
plt.rcParams['figure.figsize'] = (7.5,2.5)
plt.rcParams["animation.html"] = "jshtml"
```
```python
def show_animation(alpha=0.001):
    ## 1. 히스토리 기록을 위한 list 초기화
    loss_history = []
    yhat_history = []
    What_history = []

    ## 2. 학습 + 학습과정기록
    What= torch.tensor([[-5.0],[10.0]],requires_grad=True)
    What_history.append(What.data.tolist())
    for epoc in range(30):
        yhat=X@What ; yhat_history.append(yhat.data.tolist())
        loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())
        loss.backward()
        What.data = What.data - alpha * What.grad; What_history.append(What.data.tolist())
        What.grad = None

    ## 3. 시각화
    fig = plt.figure()
    ax1 = fig.add_subplot(1, 2, 1)
    ax2 = fig.add_subplot(1, 2, 2, projection='3d')

    #### ax1: yhat의 관점에서..
    ax1.plot(x,y,'o',label=r"$(x_i,y_i)$")
    line, = ax1.plot(x,yhat_history[0],label=r"$(x_i,\hat{y}_i)$")
    ax1.legend()
    #### ax2: loss의 관점에서..
    w0 = np.arange(-6, 11, 0.5)
    w1 = np.arange(-6, 11, 0.5)
    W1,W0 = np.meshgrid(w1,w0)
    LOSS=W0*0
    for i in range(len(w0)):
        for j in range(len(w1)):
            LOSS[i,j]=torch.sum((y-w0[i]-w1[j]*x)**2)
    ax2.plot_surface(W0, W1, LOSS, rstride=1, cstride=1, color='b',alpha=0.1)
    ax2.azim = 30  ## 3d plot의 view 조절
    ax2.dist = 8   ## 3d plot의 view 조절
    ax2.elev = 5   ## 3d plot의 view 조절
    ax2.set_xlabel(r'$w_0$')  # x축 레이블 설정
    ax2.set_ylabel(r'$w_1$')  # y축 레이블 설정
    ax2.set_xticks([-5,0,5,10])  # x축 틱 간격 설정
    ax2.set_yticks([-5,0,5,10])  # y축 틱 간격 설정
    ax2.scatter(2.5, 4, l(2.5,4), s=200, marker='*', color='red', label=r"${\bf W}=[2.5, 4]'$")
    ax2.scatter(-5, 10, l(-5,10), s=200, marker='*', color='blue')
    ax2.legend()
    def animate(epoc):
        line.set_ydata(yhat_history[epoc])
        ax2.scatter(np.array(What_history)[epoc,0],np.array(What_history)[epoc,1],loss_history[epoc],color='grey')
        fig.suptitle(f"alpha = {alpha} / epoch = {epoc}")
        return line

    ani = animation.FuncAnimation(fig, animate, frames=30)
    plt.close()
    return ani
```

```python
ani = show_animation(alpha=0.001)
```

---
- 알파가 너무 작은 경우 비효율적이다.
- 너무 큰 경우도 비효율적

## 5. SSE $\to$ MSE
- 손실함수를 sse로 설정하면 학습률 선택이 비효율적이라 mse를 써야한다.

- sse 일때 코드
```python
What = torch.tensor([[-5.0],[10.0]],requires_grad = True)
for epoc in range(30):
    # step1: yhat
    yhat = X@What
    # step2: loss
    loss = torch.sum((y-yhat)**2)
    # step3: 미분
    loss.backward()
    # step4: update
    What.data = What.data - 0.001 * What.grad
    What.grad = None
```

- mse일때 코드
```python
What = torch.tensor([[-5.0],[10.0]],requires_grad = True)
for epoc in range(30):
    # step1: yhat
    yhat = X@What
    # step2: loss
    loss = torch.sum((y-yhat)**2)/100 # torch.mean((y-yhat)**2)
    # step3: 미분
    loss.backward()
    # step4: update
    What.data = What.data - 0.1 * What.grad
    What.grad = None
```

## 6.파이토치식 코딩패턴
```python
torch.manual_seed(43052)
x,_ = torch.randn(100).sort()
eps = torch.randn(100)*0.5
X = torch.stack([torch.ones(100),x],axis=1)
W = torch.tensor([[2.5],[4.0]])
y = X@W + eps.reshape(100,1)
x = X[:,[1]]
```

### A. 기본패턴
```python
What = torch.tensor([[-5.0],[10.0]],requires_grad = True)
for epoc in range(30):
    # step1: yhat
    yhat = X@What
    # step2: loss
    loss = torch.sum((y-yhat)**2)/100
    # step3: 미분
    loss.backward()
    # step4: update
    What.data = What.data - 0.1 * What.grad
    What.grad = None
```

### loss_fn 이용
```python
What = torch.tensor([[-5.0],[10.0]],requires_grad = True)
loss_fn = torch.nn.MSELoss() # torch.nn.MSELoss는 callable object를 생성하는 함수
for epoc in range(30):
    # step1: yhat
    yhat = X@What
    # step2: loss
    #loss = torch.sum((y-yhat)**2)/100
    loss = loss_fn(yhat,y) # 여기서는 큰 상관없지만 습관적으로 yhat을 먼저넣는 연습을 하자!!
    # step3: 미분
    loss.backward()
    # step4: update
    What.data = What.data - 0.1 * What.grad
    What.grad = None
```

## 7. net 이용하기
- yhat = net(X)가 가능하게 하는 코드
- yhat = X@What과 같은 효과

```python
net = torch.nn.Linear(
    in_features=2, # X:(n,2) --> 2
    out_features=1, # yhat:(n,1) --> 1
    bias=False
)
```
```python
net.weight.data = torch.tensor([[-5.0], [10.0]]).T # .T 를 꼭 해야함.
net(X)
```
### 수정된 코드
```python
# step1을 위한 사전준비
net = torch.nn.Linear(
    in_features=2,
    out_features=1,
    bias=False
)
net.weight.data = torch.tensor([[-5.0,  10.0]])
# step2를 위한 사전준비
loss_fn = torch.nn.MSELoss()
for epoc in range(30):
    # step1: yhat
    # yhat = X@What
    yhat = net(X)
    # step2: loss
    loss = loss_fn(yhat,y)
    # step3: 미분
    loss.backward()
    # step4: update
    net.weight.data = net.weight.data - 0.1 * net.weight.grad
    net.weight.grad = None
```

## 7. optimizer 이용
```python
# step1을 위한 사전준비
net = torch.nn.Linear(
    in_features=2,
    out_features=1,
    bias=False
)
net.weight.data = torch.tensor([[-5.0,  10.0]])
# step2를 위한 사전준비
loss_fn = torch.nn.MSELoss()
# step4를 위한 사전준비
optimizr = torch.optim.SGD(net.parameters(),lr=0.1)
for epoc in range(30):
    # step1: yhat
    yhat = net(X)
    # step2: loss
    loss = loss_fn(yhat,y)
    # step3: 미분
    loss.backward()
    # step4: update
    optimizr.step()
    optimizr.zero_grad()
```

-  optimizr를 이용하여 net.weight.data = net.weight.data - 0.1 * net.weight.grad 이 부분을 간단하게 하였다.
