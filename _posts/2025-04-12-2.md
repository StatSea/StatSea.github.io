---
layout: single
title: "[ML] 기계학습 Chap 3 "
categories: [ML]
tags: [ML]
mathjax: true
---
기계학습 Chap 3 정리


---

- 차원이 높고 선형
- 차원이 높고 비선형
- 차원이 낮고 선형
- 차원이 낮고 비선형

-  이 경우를 모두 다룰 것

# 선형회귀
- 가장 간단한 형태의 지도학습 방법
- 양적변수의 예측을 위해 유용함
- Fancy하지는 않으나 여전히 유용하고 많은 학습기법들이 선형회귀모형의 일반화 또는 확장의 관점에서 이해됌

## 문제
- 예측변수와 반응변수 사이에 관계가 있는가?
- 변수간의 관계가 얼마나 강한가
- 어떤 예측변수가 반응변수에 크게 기여하는가?
- 예측변수가 반응변수에 미치는 효과를 얼마나 정확히 예측할 수 있는가?
- 반응변수의 future value를 얼마나 정확히 예측할 수 있는가?
  - 평균을 잘 알아야 한다.
- 변수간의 관계가 선형인가?
- 예측 변수들 사이에서 일종의 시너지효과가 있는가?
  - 시너지 효과 : 나머지 변수가 다른 변수에 영향을 주는 것
  - 예 : 흑백 차이가 남녀에 영향을 받는다.

# 단순선형회귀모형
- 반응변수 y와 단일 예측변수 x 사이에 근사적인 선형관계가 성립한다고 가정
- 모수에 대한 추정치를 훈련자료로부터 얻으면 주어진 예측 변수의 수준 X=x에서의 예측치 $$\hat{y}$$ 를 얻는다.

  ![image](https://github.com/user-attachments/assets/2900c2c6-cb67-41d3-a1d6-a5452462fbbd)

# 회귀계수의 추정 : 최소 제곱법
- 잘 적합하는 회귀계수를 찾는 것이 목표이므로 결과로써 주어지는 회귀 직선이 주어진 자료에 가능하면 "가까워야"한다.

`-` 최소제곱법 : 가까운 정도 측정, 잔차들의 제곱합을 최소로 함
![image](https://github.com/user-attachments/assets/938922c3-3874-4ce2-98d8-1305a6f192fc)

# 추정의 정확성 평가
- 편의 : 많은 수의 데이터셋들로부터 적합된 직선을 반복해서 얻었을 때 평균적인 직선이 실제 모형과 얼마나 다른가?
  - 단순선형회귀모형에서 최소제곱법에 의해 얻어진 직선은 불편성 성립
  - 평균적 직선이 실제 모형과 일치
  
- 표준오차 : 추정의 불확실성, 신뢰성
  - 추정량의 표준편차, 표본의 크기가 커짐에 따라 감소

# 추정의 정확성 평가
- 분산이 작으면 변동 x
- 분산이 크면 변동 o
- 직선의 경우에서도 분산이 크면 직선이 true 선보다 확 튄다.
- 직선이 평균과 비슷한 경우 = 편이가 없음
- 표본의 크기를 늘리면 분산은 줄어든다.

# 표준오차의 활용
- 표준오차는 신뢰구간 구성에 활용된다.
- $$\hat{\beta}_1$$ 의 95% 근사 신뢰구간 
 
$$
\hat{\beta}_1 \pm 2 SE(\hat{\beta}_1)
$$

- 위 구간은  $$\hat{\beta}_1$$이 근사적으로 정규분포를 따른다는 사실로부터 유도된 것이다.
-  $$\hat{\beta}_1$$ = 0을 검증할때 통계량에도 활용이 가능하다.

$$
t = \frac{\hat{\beta}_1 - 0}{SE(\hat{\beta}_1)} = \frac{\hat{\beta}_1}{SE(\hat{\beta}_1)}
$$

- t분포를 이용해 p-value 계산이 가능하다.

# 모형의 정확성 평가

`-` RSE = 오차항의 표준편차의 추정치

$$
\text{RSE} = \sqrt{ \frac{RSS}{n - 2} }
$$

- 모형의 lack of fit 측도로 활용이 가능하다.

`-` 결정계수 $$ R^2 $$ : 반응변수의 전체 변동 중 적합된 직선에 의해 설명되는 변동의 비율

$$
R^2 = 1 - \frac{RSS}{TSS}
$$

- RSS : 잔차제곱합
- TSS : 총 제곱합 , 비율측도로 Y의 scale에 무관

# 중회귀
- 반응변수 Y와 여러 예측 변수들 사이에 선형관계를 가정한다.
- 최소 제곱법에 의해 계수 추정이 가능
- 표준오차, RSS, 결정계수 등은 단순선형회귀모형과 비슷한 방식으로 계산

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k + \varepsilon
$$

## 이해해아하는 부분 ( 파이썬 결과보고 해석가능해야 )
### 변수들 간의 연관성이 존재 하나?
- y를 설명할 때 도움이 되는 변수를 써야 최소제곱합이 작아짐
- 따라서 회귀계수가 유의한지 가설 검정을 해야함
  - 사용하는 통계량 : F분포를 이용한 통계량
  - H0 : 모든 회귀계수 = 0
  - H0 이 틀렸을 경우 : F값이 커짐

## 어떤 변수가 더 중요한가?
- 의미 없는 예측변수들이 존재할 수 있고, 의미가 있더라도 변수의 개수를 축소시키는 것이 도움이 된다.
- 변수 선택을 위한 여러 방법 및 측도가 존재한다. ( $$ R^2_{adj} $$ 등)
- 변수의 개수가 p개면 가능한 모형은 총 $$ 2^p $$ 이므로 모든 모형을 적합하는 것은 비효율적이다.

`-` 전진선택, 후진제거, 단계적 선택법 등을 많이 사용한다.

  - 전진선택 : 하나씩 집어넣는 것 , 새로운 것을 넣었을 때 그 전보다 나아야 한다.
  - 후진선택 : 변수를 전부 다 쓰고 하나씩 빼기, 성능을 얼마나 올리는지 확인하고 멈추기
  - 단계적선택법 : 전진 + 후진

`-` LASSO와 같은 축소 추정법을 이용하기도 한다. 

  - 다이아몬드 영역 안으로 넣을 수 있게 하는 방법
  - 0이 아닌 것을 0으로 예측하는 근본적인 문제가 존재한다.

## 모형 적합도 
- 변수가 많으면 t검정이 문제가 될 수도 있음.
- 그래프를 이용한 시각적 확인이 필요하다

## 예측
- 계수에 대한 추정 : 주어진 예측변수의 값 -> 반응변수의 예측
- 3종류의 불확실성 존재
  - 계수의 추정으로부터 오는 부정확성 : reducible error
  - 선형성 가정은 실재에 대한 근사이므로 그 차이에서 오는 부정확성 : model bias
  - f를 정확히 알아도 오차로부터 발생하는 부정확성 : irreducible error

`-` 평균반응에 대한 추정 vs 개별 반응에 대한 추정

![image](https://github.com/user-attachments/assets/a2cf118d-0cfb-4c5c-8436-ca6bcb5aa6db)

## Leverage
- 훈련 데이터의 모든 관측치가 동일한 비중의 기여를 하는 것은 아니다.
- 각 관측치의 모형에 대한 영향력을 평가하는 측도 개발
  - 단순 선형 회귀 leverage
    
$$
h_i = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{j=1}^{n} (x_j - \bar{x})^2}
$$

  - 중회귀 leverage

$$
h_i = x_i^T (X^T X)^{-1} x_i
$$

- 평균하고 거리가 먼 데이터들은 추정직선에 영향을 많이 주는 것을 알 수 있다.
- leverage : 해당 관측값의 x값이 평균으로부터 얼마나 멀리 떨어져 있는지를 나타내는 측도

## $$ R^2 $$ 을 사용하는 이유

![image](https://github.com/user-attachments/assets/c6281c9c-78c0-434b-8829-fa76cf132faf)

## 범주형 변수의 가변수화
- 어떤 설명변수가 범주형 변수일때 수치적으로 처리하는 방법
- 변수 V 가 C개의 범주를 가지고 있다고 하면 각각의 범주를 나타내게 함

`-` 예시 

![image](https://github.com/user-attachments/assets/a2355d7a-f7a8-48ef-ab22-92deb5136775)

# 선형 모형의 확장
- 가법성 제거 : 상호작용 포함
$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \varepsilon
$$

- 두 변수가 따로가 아니라 상호작용 하에서 y에 영향을 주게 된다.
- 이런 경우가 교호작용이다.
- 두 변수를 곱하는 이유?
  - 다른 변수의 값에 의해 x1의 기울기가 바뀌기 때문이다.
  - 그래서 서로 곱해서 추가하는 것

- 비 선형 관계
$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1^2 + \varepsilon
$$
  - 데이터에 곡률이 들어간 경우는 문제 해결이 어렵기 때문에 특화된 방법이 필요하다.
  - 

## 잠재적 문제
- 예측변수와 반응변수간의 비선형관계
- 오차들간의 상관성 => 더빈왓슨 (시계열)
- 오차들 분산의 이질성
  - 등분산성 만족 x => 로그변환, 분산모형화 사용
- 이상치
  - 로봇인지 사람인지?
- 영향치
  - 밖에서 영향을 많이 주는 것
- 다중공선성
  - 서로 corr가 강해서 생기는 문제
  - X2 = X1 인 경우 실제 정보가 2개가 있는것 같지만 사실 1개만 있음. => 다중공선성 문제
  - X2 와 X1이 비슷한 경우도 유사한 상황이 발생한다.
    - Ridge , DNN 로 해결하거나 새로운 데이터로 변환시키기

### 광고데이터 예제
#### 광고비와 판매량 사이에 연관성이 있는가?

```R
ad = read.csv('data/Advertising.csv')
options(digits=4)
# 종속변수는 sales, 독립변수는 tv, radio , newspaper
fit = lm(sales ~ Tv + radio + newspaper , data = ad)
fit1 = summary(fit)
(pvlaue =1 - pf(fit1$fstatistic[1] , fit1$fstatistic[2] , fit1$fstatistic[3]))
```
|인덱스|	의미|
|---|---|
|fit1$fstatistic[1]	|F 값 (F-statistic)|
|fit1$fstatistic[2]	|분자 자유도 (df1 = 모델의 독립 변수 개수)|
|fit1$fstatistic[3]	|분모 자유도 (df2 = 잔차의 자유도 = n - p - 1)|

- pvalue = 1 - pf(F값, 분자 자유도, 분모 자유도)

- 검정결과 p-value가 매우 작아 연관성이 있다는 증거가 확실히 관측된다.

#### 연관성의 정도는?
```R
(RSE = sqrt(sm(fit$residuals^2) / (fit$df.residual)))
(R2 = fit1$r.squared)
```
- 1.685 , 0.8972가 출력된다.
- 예측값이 실제값에서 평균적으로 1.685 정도 오차가 있다.
- 전체 변동성 중 약 89.72%를 모델이 설명하고 있다.

#### 어떤 media가 판매량에 영향을 미치는가?
```R
fit1$coefficients[,4]
```
- 신문은 연관성이 관측되지 않음을 알 수 있었음.

#### 신뢰구간
```R
cbind(fit1$coefficients[,1] - 2*fit1$coefficients[,2],
      fit1$coefficients[,1] + 2*fit1$coefficients[,2])
```
- 신문에 대해서는 신뢰구간이 0을 포함하고 있었다. -> 모수가 0이 될 수 있어 검정 통과 x
- fit1$coefficients[,1]: 각 회귀계수의 추정값
- fit1$coefficients[,2]: 각 계수의 표준오차
- 선형성이 의심됌

#### 상호작용 존재? ( 통계적 유의성 확인 )
```R
fit2 = lm(sales ~ TV +radio, data = ad)
fit3 = lm(sales ~ TV * radio, data = ad)
c(summary(fit2)$r.squared, summary(fit3)$r.squared)
summary(fit3)$coefficients[4,]
```
fit3 결정계수가 더 높다.

- fit2 : tv와 라디오 광고가 독립적으로 sales에 영향을 미친다
- fit3 : tv와 라디오 광고 사이에 교호작용이 있다.
- 두 모형의 결정계수 출력 => 결과를 보고 모형 설명력 판단
- 교호 작용 항의 추정값, 표준오차, t값, p값 한번에 보기

#### 가변수 예제
- 가변수 : 범주형 변수를 수치형(0,1) 으로 변환한 변수
![image](https://github.com/user-attachments/assets/4ec40fec-ab41-4b18-b637-cc32a2563cad)

# 선형회귀 확장모형에 대한 검정
- 모형 M1 = 변수 p개 사용
- 모형 M2 = 변수 q개 사용
  - p < q
  - 모형 M2는 M1이 사용한 모든 변수 포함
- 어느정도의 RSS 감소가 있어야 뫃여을 확장하는 것이 통계적으로 유의한 변화를 주었다고 할 것인가?

$$
F = \frac{RSS(m_1) - RSS(m_2)}{q - p} \Big/ \frac{RSS(m_2)}{n - q - 1}
\sim F_{q - p,\ n - q - 1}
$$

- 만약 reduce 모델과 full 모델의 값이 같으면 reduce 모델을 쓰기
- H0 : M1 = M2 -> 추가된 변수들이 설명력 향상에 기여하지 않는다.

# 부록 : 경사하강법
- 일차 편미분만을 이용하여 최적화
- 신경망 모형 적합시 주로 이용되며 수많은 variation이 존재
- 0개 되는 점이 여러개라 최소인 포인트를 찾기 힘들다.
