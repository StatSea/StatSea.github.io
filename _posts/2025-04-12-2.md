---
layout: single
title: "[ML] 기계학습 Chap 3 "
categories: [ML]
tags: [ML]
mathjax: true
---
기계학습 Chap 3 정리


---

- 차원이 높고 선형
- 차원이 높고 비선형
- 차원이 낮고 선형
- 차원이 낮고 비선형

-  이 경우를 모두 다룰 것

# 선형회귀
- 가장 간단한 형태의 지도학습 방법
- 양적변수의 예측을 위해 유용함
- Fancy하지는 않으나 여전히 유용하고 많은 학습기법들이 선형회귀모형의 일반화 또는 확장의 관점에서 이해됌

## 문제
- 예측변수와 반응변수 사이에 관계가 있는가?
- 변수간의 관계가 얼마나 강한가
- 어떤 예측변수가 반응변수에 크게 기여하는가?
- 예측변수가 반응변수에 미치는 효과를 얼마나 정확히 예측할 수 있는가?
- 반응변수의 future value를 얼마나 정확히 예측할 수 있는가?
  - 평균을 잘 알아야 한다.
- 변수간의 관계가 선형인가?
- 예측 변수들 사이에서 일종의 시너지효과가 있는가?
  - 시너지 효과 : 나머지 변수가 다른 변수에 영향을 주는 것
  - 예 : 흑백 차이가 남녀에 영향을 받는다.

# 단순선형회귀모형
- 반응변수 y와 단일 예측변수 x 사이에 근사적인 선형관계가 성립한다고 가정
- 모수에 대한 추정치를 훈련자료로부터 얻으면 주어진 예측 변수의 수준 X=x에서의 예측치 $$\hat{y}$$ 를 얻는다.

  ![image](https://github.com/user-attachments/assets/2900c2c6-cb67-41d3-a1d6-a5452462fbbd)

# 회귀계수의 추정 : 최소 제곱법
- 잘 적합하는 회귀계수를 찾는 것이 목표이므로 결과로써 주어지는 회귀 직선이 주어진 자료에 가능하면 "가까워야"한다.

`-` 최소제곱법 : 가까운 정도 측정, 잔차들의 제곱합을 최소로 함
![image](https://github.com/user-attachments/assets/938922c3-3874-4ce2-98d8-1305a6f192fc)

# 추정의 정확성 평가
- 편의 : 많은 수의 데이터셋들로부터 적합된 직선을 반복해서 얻었을 때 평균적인 직선이 실제 모형과 얼마나 다른가?
  - 단순선형회귀모형에서 최소제곱법에 의해 얻어진 직선은 불편성 성립
  - 평균적 직선이 실제 모형과 일치
  
- 표준오차 : 추정의 불확실성, 신뢰성
  - 추정량의 표준편차, 표본의 크기가 커짐에 따라 감소

# 추정의 정확성 평가
- 분산이 작으면 변동 x
- 분산이 크면 변동 o
- 직선의 경우에서도 분산이 크면 직선이 true 선보다 확 튄다.
- 직선이 평균과 비슷한 경우 = 편이가 없음
- 표본의 크기를 늘리면 분산은 줄어든다.

# 표준오차의 활용
- 표준오차는 신뢰구간 구성에 활용된다.
- $$\hat{\beta}_1$$ 의 95% 근사 신뢰구간 
 
$$
\hat{\beta}_1 \pm 2 SE(\hat{\beta}_1)
$$

- 위 구간은  $$\hat{\beta}_1$$이 근사적으로 정규분포를 따른다는 사실로부터 유도된 것이다.
-  $$\hat{\beta}_1$$ = 0을 검증할때 통계량에도 활용이 가능하다.

$$
t = \frac{\hat{\beta}_1 - 0}{SE(\hat{\beta}_1)} = \frac{\hat{\beta}_1}{SE(\hat{\beta}_1)}
$$

- t분포를 이용해 p-value 계산이 가능하다.

# 모형의 정확성 평가

`-` RSE = 오차항의 표준편차의 추정치

$$
\text{RSE} = \sqrt{ \frac{RSS}{n - 2} }
$$

- 모형의 lack of fit 측도로 활용이 가능하다.

`-` 결정계수 $$ R^2 $$ : 반응변수의 전체 변동 중 적합된 직선에 의해 설명되는 변동의 비율

$$
R^2 = 1 - \frac{RSS}{TSS}
$$

- RSS : 잔차제곱합
- TSS : 총 제곱합 , 비율측도로 Y의 scale에 무관

# 중회귀
- 반응변수 Y와 여러 예측 변수들 사이에 선형관계를 가정한다.
- 최소 제곱법에 의해 계수 추정이 가능
- 표준오차, RSS, 결정계수 등은 단순선형회귀모형과 비슷한 방식으로 계산

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k + \varepsilon
$$

## 이해해아하는 부분 ( 파이썬 결과보고 해석가능해야 )
### 변수들 간의 연관성이 존재 하나?
- y를 설명할 때 도움이 되는 변수를 써야 최소제곱합이 작아짐
- 따라서 회귀계수가 유의한지 가설 검정을 해야함
  - 사용하는 통계량 : F분포를 이용한 통계량
  - H0 : 모든 회귀계수 = 0
  - H0 이 틀렸을 경우 : F값이 커짐

## 어떤 변수가 더 중요한가?
- 의미 없는 예측변수들이 존재할 수 있고, 의미가 있더라도 변수의 개수를 축소시키는 것이 도움이 된다.
- 변수 선택을 위한 여러 방법 및 측도가 존재한다. ( $$ R^2_{adj} $$ 등)
- 변수의 개수가 p개면 가능한 모형은 총 $$ 2^p $$ 이므로 모든 모형을 적합하는 것은 비효율적이다.

`-` 전진선택, 후진제거, 단계적 선택법 등을 많이 사용한다.

  - 전진선택 : 하나씩 집어넣는 것 , 새로운 것을 넣었을 때 그 전보다 나아야 한다.
  - 후진선택 : 변수를 전부 다 쓰고 하나씩 빼기, 성능을 얼마나 올리는지 확인하고 멈추기
  - 단계적선택법 : 전진 + 후진

`-` LASSO와 같은 축소 추정법을 이용하기도 한다. 

  - 다이아몬드 영역 안으로 넣을 수 있게 하는 방법
  - 0이 아닌 것을 0으로 예측하는 근본적인 문제가 존재한다.

## 모형 적합도 
- 변수가 많으면 t검정이 문제가 될 수도 있음.
- 그래프를 이용한 시각적 확인이 필요하다

## 예측
- 계수에 대한 추정 : 주어진 예측변수의 값 -> 반응변수의 예측
- 3종류의 불확실성 존재
  - 계수의 추정으로부터 오는 부정확성 : reducible error
  - 선형성 가정은 실재에 대한 근사이므로 그 차이에서 오는 부정확성 : model bias
  - f를 정확히 알아도 오차로부터 발생하는 부정확성 : irreducible error

`-` 평균반응에 대한 추정 vs 개별 반응에 대한 추정

![image](https://github.com/user-attachments/assets/a2cf118d-0cfb-4c5c-8436-ca6bcb5aa6db)

## Leverage
- 훈련 데이터의 모든 관측치가 동일한 비중의 기여를 하는 것은 아니다.
- 각 관측치의 모형에 대한 영향력을 평가하는 측도 개발
  - 단순 선형 회귀 leverage
    
$$
h_i = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{j=1}^{n} (x_j - \bar{x})^2}
$$

  - 중회귀 leverage

$$
h_i = x_i^T (X^T X)^{-1} x_i
$$

- 평균하고 거리가 먼 데이터들은 추정직선에 영향을 많이 주는 것을 알 수 있다.
- leverage : 해당 관측값의 x값이 평균으로부터 얼마나 멀리 떨어져 있는지를 나타내는 측도
