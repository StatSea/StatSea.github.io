---
layout: single
title: "[DL] 딥러닝 03wk2"
categories: [DL]
tags: [DL]
mathjax: true
---
딥러닝 03wk2 이해하기

---
# 기본 세팅

```python
import torch
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
plt.rcParams['figure.figsize'] = (4.5, 3.0)
```
---
# 3. 로지스틱 최초적합
## A. 로지스틱 모형

- 우리가 예측하려는 것
  - 회귀모형 : 정규분포의 평균
    - 예측값 : $$\hat{w}_0 + \hat{w}_1x_i$$
  - 로지스틱 : 베르누이의 평균
    - 예측값 : $$\frac{\exp(\hat{w}_0+\hat{w}_1x_i)}{1+\exp(\hat{w}_0+\hat{w}_1x_i)}$$
---  

## B. 데이터
```python
torch.manual_seed(43052)
x = torch.linspace(-1,1,2000).reshape(2000,1)
w0,w1 = -1, 5
prob = torch.exp(w0+w1*x) / (1+torch.exp(w0+w1*x))
y = torch.bernoulli(prob)
```
```python
plt.plot(x,y,'.',alpha=0.03)
plt.plot(x[0],y[0],'.',label=r"$(x_i,y_i)$",color="C0")
plt.plot(x,prob,'--r',label=r"prob (true, unknown) = $\frac{exp(-1+5x)}{1+exp(-1+5x)}$")
plt.legend()
```
![image](https://github.com/user-attachments/assets/a82926ad-97f8-4ee4-8783-64dc0072dc38)
---

## C.Step1 : net 설계 (모델링)
`-` 최초의 곡선 그리기
   - 최초의직선: $$\hat{y}_i= \hat{w}_0+\hat{w}_1x_i$$ 에서 아무 $$\hat{w}_0$$, $$\hat{w}_1$$ 을 설정하면 된다.
  - 최초의곡선: $$\hat{y}_i= \frac{\exp(\hat{w}_0+\hat{w}_1x_i)}{1+\exp(\hat{w}_0+\hat{w}_1x_i)}=\frac{1}{1+\exp(-\hat{w}_0-\hat{w}_1x_i)}$$ 에서 아무 $$\hat{w}_0$$, $$\hat{w}_1$$ 을 설정하면 된다.

- 일단은 초기 설정값을 $$\hat{w}_0 = -0.8$, $\hat{w}_1 = -0.3$$ 으로 하자. (실제값은 $$w_0=-1$$, $$w_1=5$$ 이다)

### 방법 1. l1 , sigmoid
```python
# w0hat + w1hat*x
l1 = torch.nn.Linear(1,1)
l1.weight.data = torch.tensor([[-0.3]])
l1.bias.data = torch.tensor([-0.8])

- sigmoid 함수 생성
```python
def sigmoid(x):
    return torch.exp(x)/(1+torch.exp(x))
```
- 시각화 하기
```python
plt.plot(x,y,'.',alpha=0.03)
plt.plot(x[0],y[0],'o',label=r"$(x_i,y_i)$",color="C0")
plt.plot(x,prob,'--r',label=r"prob (true, unknown) = $\frac{exp(-1+5x)}{1+exp(-1+5x)}$")
plt.plot(x,sigmoid(l1(x)).data,'--b', label=r"prob (estimated) = $(x_i,\hat{y}_i)$ -- first curve")
plt.legend()
```
![image](https://github.com/user-attachments/assets/88605ca5-cef5-438b-88d7-f7cbdd348b32)

### 방법 2. l1, a1
```python
l1 = torch.nn.Linear(1,1)
l1.weight.data = torch.tensor([[-0.3]])
l1.bias.data = torch.tensor([-0.8])
a1 = torch.nn.Sigmoid()

# 시각화
plt.plot(x,y,'.',alpha=0.03)
plt.plot(x[0],y[0],'o',label=r"$(x_i,y_i)$",color="C0")
plt.plot(x,prob,'--r',label=r"prob (true, unknown) = $\frac{exp(-1+5x)}{1+exp(-1+5x)}$")
plt.plot(x,a1(l1(x)).data,'--b', label=r"prob (estimated) = $(x_i,\hat{y}_i)$ -- first curve with $(a_1 \circ l_1)(x)$")
plt.legend()
```
- 방법 1과 같은 답이 나온다.
![image](https://github.com/user-attachments/assets/2098d86a-6c38-4243-ba26-0bff0e74c2c8)

### 방법 3. l1 , a1만들고 -> net
- $${\bf x} \overset{l_1}{\to} {\bf u} \overset{a_1}{\to} {\bf v} = \hat{\bf y}$$ 로 된 구조
- $$(a_1\circ l_1)({\bf x}) := net({\bf x})$$ 로 바꾸기

```python
l1 = torch.nn.Linear(1,1)
l1.weight.data = torch.tensor([[-0.3]])
l1.bias.data = torch.tensor([-0.8])
a1 = torch.nn.Sigmoid()
# l1 먼저, 그 다음 a1
net = torch.nn.Sequential(l1,a1)
```
- 동일한 결과가 나온다.
- net의 구조는 어떻게 되어 있을까?
```python
l1 is net[0]
a1 is net[1]
```
- 전부 true로 출력된다.

### 방법 4. net을 바로 만들기

```python
net = torch.nn.Sequential(
    torch.nn.Linear(1,1),
    torch.nn.Sigmoid()
)
net[0].weight.data = torch.tensor([[-0.3]])
net[0].bias.data = torch.tensor([-0.8])
yhat = net(x)
```

## D. Step 1~4
```python
net = torch.nn.Sequential(
    torch.nn.Linear(in_features=1, out_features=1),
    torch.nn.Sigmoid()
)
l1, a1 = net
l1.weight.data = torch.tensor([[-0.3]])
l1.bias.data = torch.tensor([-0.8])
optimizr = torch.optim.SGD(net.parameters(),lr=0.25)
#---#
for epoc in range(100):
    ## 1
    yhat = net(x)
    ## 2
    loss = torch.mean((y-yhat)**2)
    ## 3
    loss.backward()
    ## 4
    optimizr.step()
    optimizr.zero_grad()
```

|변수|의미|직접 사용|
|----|----|----|
|l1|	Linear 계층 |가중치 설정 위해 필요|	
|a1|	Sigmoid 계층|자동 호출됨|

### 시각화 
```python
plt.plot(x,y,'.',alpha=0.05)
plt.plot(x,prob,'--r')
plt.plot(x,yhat.data,'--b')
plt.title('after 100 epochs')
```

- 빨간색 선은 이상적인 선
- 우리가 맞춰나가고 있는건 파란색 선
![image](https://github.com/user-attachments/assets/97d98bd5-1d3f-44d0-86cc-a42bd09d47d4)

- 에폭을 늘리면 될 것 같아 보임

```python
for epoc in range(4900):
    ## 1
    yhat = net(x)
    ## 2
    loss = torch.mean((y-yhat)**2)
    ## 3
    loss.backward()
    ## 4
    optimizr.step()
    optimizr.zero_grad()
```
```python
plt.plot(x,y,'.',alpha=0.05)
plt.plot(x,prob,'--r')
plt.plot(x,yhat.data,'--b')
plt.title('after 5000 epochs')
```
![image](https://github.com/user-attachments/assets/e8f6375a-6721-4967-9e5f-98c4f1434985)

# 학습과정 시각화 및 문제 인식
## A. 시각화를 위한 준비
```python
def plot_loss(loss_fn, ax=None, Wstar=[-1,5]):
    w0hat,w1hat =torch.meshgrid(torch.arange(-10,3,0.1),torch.arange(-1,10,0.1),indexing='ij')
    w0hat = w0hat.reshape(-1)
    w1hat = w1hat.reshape(-1)
    def l(w0hat,w1hat):
        yhat = torch.exp(w0hat+w1hat*x)/(1+torch.exp(w0hat+w1hat*x))
        return loss_fn(yhat,y)
    loss = list(map(l,w0hat,w1hat))
    #---#
    if ax is None:
        fig = plt.figure()
        ax = fig.add_subplot(1,1,1,projection='3d')
    ax.scatter(w0hat,w1hat,loss,s=0.001)
    ax.scatter(w0hat[::20],w1hat[::20],loss[::20],s=0.1,color='C0')
    w0star,w1star = np.array(Wstar).reshape(-1)
    ax.scatter(w0star,w1star,l(w0star,w1star),s=200,marker='*',color='red',label=f"W=[{w0star:.1f},{w1star:.1f}]")
    #---#
    ax.elev = 15
    ax.dist = -20
    ax.azim = 75
    ax.legend()
    ax.set_xlabel(r'$w_0$')  # x축 레이블 설정
    ax.set_ylabel(r'$w_1$')  # y축 레이블 설정
    ax.set_xticks([-10,-5,0])  # x축 틱 간격 설정
    ax.set_yticks([-10,0,10])  # y축 틱 간격 설정
```

### plot_loss() - 손실 지형 시각화 함수
- (w₀, w₁)의 조합에 대해 손실 함수 값을 계산해 3D로 그림.
- 이 손실 함수는 시그모이드 로지스틱 예측을 기반으로 함
- 이 예측값과 y와의 오차로 loss를 계산

- 주요 동작
  - torch.meshgrid()로 w₀, w₁값을 쭉 생성.
  - 각각에 대해 loss_fn을 적용해 손실 값 계산
  - 결과를 3D scatter로 그림
  - Wstar는 실제 true weight로 빨간 별(★) 마킹
  
```python
def _learn_and_record(net, loss_fn, optimizr):
    yhat_history = []
    loss_history = []
    What_history = []
    Whatgrad_history = []
    What_history.append([net[0].bias.data.item(), net[0].weight.data.item()])
    for epoc in range(100):
        ## step1
        yhat = net(x)
        ## step2
        loss = loss_fn(yhat,y)
        ## step3
        loss.backward()
        ## step4
        optimizr.step()
        ## record
        if epoc % 5 ==0:
            yhat_history.append(yhat.reshape(-1).data.tolist())
            loss_history.append(loss.item())
            What_history.append([net[0].bias.data.item(), net[0].weight.data.item()])
            Whatgrad_history.append([net[0].bias.grad.item(), net[0].weight.grad.item()])
        optimizr.zero_grad()

    return yhat_history, loss_history, What_history, Whatgrad_history

def show_animation(net, loss_fn, optimizr):
    yhat_history,loss_history,What_history,Whatgrad_history = _learn_and_record(net,loss_fn,optimizr)

    fig = plt.figure(figsize=(7.5,3.5))
    ax1 = fig.add_subplot(1, 2, 1)
    ax2 = fig.add_subplot(1, 2, 2, projection='3d')
    ## ax1: 왼쪽그림
    ax1.scatter(x,y,alpha=0.01)
    ax1.scatter(x[0],y[0],color='C0',label=r"observed data = $(x_i,y_i)$")
    ax1.plot(x,prob,'--',label=r"prob (true) = $(x_i,\frac{exp(-1+5x_i)}{1+exp(-1+5x_i)})$")
    line, = ax1.plot(x,yhat_history[0],'--',label=r"prob (estimated) = $(x_i,\hat{y}_i)$")
    ax1.legend()
    ## ax2: 오른쪽그림
    plot_loss(loss_fn,ax2)
    ax2.scatter(np.array(What_history)[0,0],np.array(What_history)[0,1],loss_history[0],color='blue',s=200,marker='*')
    def animate(epoc):
        line.set_ydata(yhat_history[epoc])
        w0hat = np.array(What_history)[epoc,0]
        w1hat = np.array(What_history)[epoc,1]
        w0hatgrad = np.array(Whatgrad_history)[epoc,0]
        w1hatgrad = np.array(Whatgrad_history)[epoc,1]
        ax2.scatter(w0hat,w1hat,loss_history[epoc],color='grey')
        ax2.set_title(f"What.grad=[{w0hatgrad:.4f},{w1hatgrad:.4f}]",y=0.8)
        fig.suptitle(f"epoch={epoc*5} // What=[{w0hat:.2f},{w1hat:.2f}] // Loss={loss_fn.__class__.__name__} // Opt={optimizr.__class__.__name__}")
        return line
    ani = animation.FuncAnimation(fig, animate, frames=20)
    plt.close()
    return ani
```

### _learn_and_record() – 학습 + 기록 함수
- net, loss_fn, optimizr를 입력으로 받아서, 100 에폭 동안 학습.
- 5 에폭마다 다음 값들을 저장함:
  - 예측값 yhat
  - 손실값 loss
  - 가중치 w₀, w₁ (bias, weight)
  - gradient 값 (미분값)
  
- 기록된 결과:
  - yhat_history: 예측값 시퀀스
  - loss_history: 손실값 시퀀스
  - What_history: 각 시점의 weight 값 ([w₀, w₁])
  - Whatgrad_history: 각 시점의 gradient 값 ([dw₀, dw₁])

### show_animation() – 훈련 과정 애니메이션
- 구성
  - 좌측(ax1)에는 실제 y와 예측값 변화 선 그래프 , 우측(ax2)에는 손실 공간에서 weight가 움직이는 궤적
- 핵심 동작:
  - _learn_and_record()로 학습 기록
  - plot_loss()로 3D 손실 지형 표시
  - FuncAnimation을 통해 에폭별로:
    - 예측 선 yhat 업데이트
    - weight 포인트를 손실 공간에 추가
    - 제목에 현재 에폭/weight/gradient/loss/optimizer 등 정보 표시

- 결과:
  - 직관적으로 학습이 어떻게 진행되는지 확인 가능
  - weight가 손실을 최소화하는 방향으로 움직임
  - 예측 곡선이 실제 데이터에 점점 가까워짐


### 함수 사용법
```python
loss_fn = torch.nn.MSELoss()
plot_loss(loss_fn)
```
![image](https://github.com/user-attachments/assets/99e438d7-1753-47ba-a222-a277f68886b7)


- 직선 그래프와 같이 보여주려면?
```python
torch.manual_seed(42)
net = torch.nn.Sequential(
    torch.nn.Linear(1,1),
    torch.nn.Sigmoid()
)
loss_fn = torch.nn.MSELoss()
optimizr = torch.optim.SGD(net.parameters(),lr=0.25)
show_animation(net,loss_fn,optimizr)
```

![image](https://github.com/user-attachments/assets/b4a9d5ae-68a5-4b9c-820e-70ee8635c1b2)

---

## B. 좋은 초기값
```python
net[0].bias.data = torch.tensor([-0.8])
net[0].weight.data = torch.tensor([[-0.3]])
```
- 이렇게 설정하면 epoc을 늘릴수록 빨간점에 가까워 진다.

## C. 가능성 있는 초기값
```python
net[0].bias.data = torch.tensor([-3.0])
net[0].weight.data = torch.tensor([[-1.0]])
```
- 이 경우도 epoc을 많이 늘릴수록 빨간점에 가까워질 희망이 보인다. 

## D. 최악의 초기값
```python
net[0].bias.data = torch.tensor([-10.0])
net[0].weight.data = torch.tensor([[-1.0]])
```
- 희망이 없어보인다..

# 손실함수의 개선
## BCE Loss 를 사용하여 학습
`-` BEC loss = $$ - \sum_{i=1}^{n} \big(y_i\log(\hat{y}_i)+(1-y_i)\log(1-\hat{y}_i)\big)$$




