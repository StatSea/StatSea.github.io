---
layout: single
title: "[DL] 딥러닝 03wk2"
categories: [DL]
tags: [DL]
mathjax: true
---
딥러닝 03wk2 이해하기

---
# 기본 세팅

```python
import torch
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
plt.rcParams['figure.figsize'] = (4.5, 3.0)
```
---
# 3. 로지스틱 최초적합
## A. 로지스틱 모형

- 우리가 예측하려는 것
  - 회귀모형 : 정규분포의 평균
    - 예측값 : $$\hat{w}_0 + \hat{w}_1x_i$$
  - 로지스틱 : 베르누이의 평균
    - 예측값 : $$\frac{\exp(\hat{w}_0+\hat{w}_1x_i)}{1+\exp(\hat{w}_0+\hat{w}_1x_i)}$$
---  

## B. 데이터
```python
torch.manual_seed(43052)
x = torch.linspace(-1,1,2000).reshape(2000,1)
w0,w1 = -1, 5
prob = torch.exp(w0+w1*x) / (1+torch.exp(w0+w1*x))
y = torch.bernoulli(prob)
```
```python
plt.plot(x,y,'.',alpha=0.03)
plt.plot(x[0],y[0],'.',label=r"$(x_i,y_i)$",color="C0")
plt.plot(x,prob,'--r',label=r"prob (true, unknown) = $\frac{exp(-1+5x)}{1+exp(-1+5x)}$")
plt.legend()
```
![image](https://github.com/user-attachments/assets/a82926ad-97f8-4ee4-8783-64dc0072dc38)
---

## C.Step1 : net 설계 (모델링)
`-` 최초의 곡선 그리기
   - 최초의직선: $$\hat{y}_i= \hat{w}_0+\hat{w}_1x_i$$ 에서 아무 $$\hat{w}_0$$, $$\hat{w}_1$$ 을 설정하면 된다.
  - 최초의곡선: $$\hat{y}_i= \frac{\exp(\hat{w}_0+\hat{w}_1x_i)}{1+\exp(\hat{w}_0+\hat{w}_1x_i)}=\frac{1}{1+\exp(-\hat{w}_0-\hat{w}_1x_i)}$$ 에서 아무 $$\hat{w}_0$$, $$\hat{w}_1$$ 을 설정하면 된다.

- 일단은 초기 설정값을 $$\hat{w}_0 = -0.8$, $\hat{w}_1 = -0.3$$ 으로 하자. (실제값은 $$w_0=-1$$, $$w_1=5$$ 이다)

### 방법 1. l1 , sigmoid
```python
# w0hat + w1hat*x
l1 = torch.nn.Linear(1,1)
l1.weight.data = torch.tensor([[-0.3]])
l1.bias.data = torch.tensor([-0.8])

- sigmoid 함수 생성
```python
def sigmoid(x):
    return torch.exp(x)/(1+torch.exp(x))
```
- 시각화 하기
```python
plt.plot(x,y,'.',alpha=0.03)
plt.plot(x[0],y[0],'o',label=r"$(x_i,y_i)$",color="C0")
plt.plot(x,prob,'--r',label=r"prob (true, unknown) = $\frac{exp(-1+5x)}{1+exp(-1+5x)}$")
plt.plot(x,sigmoid(l1(x)).data,'--b', label=r"prob (estimated) = $(x_i,\hat{y}_i)$ -- first curve")
plt.legend()
```
![image](https://github.com/user-attachments/assets/88605ca5-cef5-438b-88d7-f7cbdd348b32)

### 방법 2. l1, a1
```python
l1 = torch.nn.Linear(1,1)
l1.weight.data = torch.tensor([[-0.3]])
l1.bias.data = torch.tensor([-0.8])
a1 = torch.nn.Sigmoid()

# 시각화
plt.plot(x,y,'.',alpha=0.03)
plt.plot(x[0],y[0],'o',label=r"$(x_i,y_i)$",color="C0")
plt.plot(x,prob,'--r',label=r"prob (true, unknown) = $\frac{exp(-1+5x)}{1+exp(-1+5x)}$")
plt.plot(x,a1(l1(x)).data,'--b', label=r"prob (estimated) = $(x_i,\hat{y}_i)$ -- first curve with $(a_1 \circ l_1)(x)$")
plt.legend()
```
- 방법 1과 같은 답이 나온다.
![image](https://github.com/user-attachments/assets/2098d86a-6c38-4243-ba26-0bff0e74c2c8)

### 방법 3. l1 , a1만들고 -> net
- $${\bf x} \overset{l_1}{\to} {\bf u} \overset{a_1}{\to} {\bf v} = \hat{\bf y}$$ 로 된 구조
- $$(a_1\circ l_1)({\bf x}) := net({\bf x})$$ 로 바꾸기

```python
l1 = torch.nn.Linear(1,1)
l1.weight.data = torch.tensor([[-0.3]])
l1.bias.data = torch.tensor([-0.8])
a1 = torch.nn.Sigmoid()
# l1 먼저, 그 다음 a1
net = torch.nn.Sequential(l1,a1)
```
- 동일한 결과가 나온다.
- net의 구조는 어떻게 되어 있을까?
```python
l1 is net[0]
a1 is net[1]
```
- 전부 true로 출력된다.

### 방법 4. net을 바로 만들기

```python
net = torch.nn.Sequential(
    torch.nn.Linear(1,1),
    torch.nn.Sigmoid()
)
net[0].weight.data = torch.tensor([[-0.3]])
net[0].bias.data = torch.tensor([-0.8])
yhat = net(x)
```

## D. Step 1~4
```python
net = torch.nn.Sequential(
    torch.nn.Linear(in_features=1, out_features=1),
    torch.nn.Sigmoid()
)
l1, a1 = net
l1.weight.data = torch.tensor([[-0.3]])
l1.bias.data = torch.tensor([-0.8])
optimizr = torch.optim.SGD(net.parameters(),lr=0.25)
#---#
for epoc in range(100):
    ## 1
    yhat = net(x)
    ## 2
    loss = torch.mean((y-yhat)**2)
    ## 3
    loss.backward()
    ## 4
    optimizr.step()
    optimizr.zero_grad()
```

|변수|의미|직접 사용|
|----|----|----|
|l1|	Linear 계층 |가중치 설정 위해 필요|	
|a1|	Sigmoid 계층|자동 호출됨|

### 시각화 
```python
plt.plot(x,y,'.',alpha=0.05)
plt.plot(x,prob,'--r')
plt.plot(x,yhat.data,'--b')
plt.title('after 100 epochs')
```

- 빨간색 선은 이상적인 선
- 우리가 맞춰나가고 있는건 파란색 선
![image](https://github.com/user-attachments/assets/97d98bd5-1d3f-44d0-86cc-a42bd09d47d4)

- 에폭을 늘리면 될 것 같아 보임

```python
for epoc in range(4900):
    ## 1
    yhat = net(x)
    ## 2
    loss = torch.mean((y-yhat)**2)
    ## 3
    loss.backward()
    ## 4
    optimizr.step()
    optimizr.zero_grad()
```
```python
plt.plot(x,y,'.',alpha=0.05)
plt.plot(x,prob,'--r')
plt.plot(x,yhat.data,'--b')
plt.title('after 5000 epochs')
```
![image](https://github.com/user-attachments/assets/e8f6375a-6721-4967-9e5f-98c4f1434985)

# 학습과정 시각화 및 문제 인식
## A. 시각화를 위한 준비
```python
def plot_loss(loss_fn, ax=None, Wstar=[-1,5]):
    w0hat,w1hat =torch.meshgrid(torch.arange(-10,3,0.1),torch.arange(-1,10,0.1),indexing='ij')
    w0hat = w0hat.reshape(-1)
    w1hat = w1hat.reshape(-1)
    def l(w0hat,w1hat):
        yhat = torch.exp(w0hat+w1hat*x)/(1+torch.exp(w0hat+w1hat*x))
        return loss_fn(yhat,y)
    loss = list(map(l,w0hat,w1hat))
    #---#
    if ax is None:
        fig = plt.figure()
        ax = fig.add_subplot(1,1,1,projection='3d')
    ax.scatter(w0hat,w1hat,loss,s=0.001)
    ax.scatter(w0hat[::20],w1hat[::20],loss[::20],s=0.1,color='C0')
    w0star,w1star = np.array(Wstar).reshape(-1)
    ax.scatter(w0star,w1star,l(w0star,w1star),s=200,marker='*',color='red',label=f"W=[{w0star:.1f},{w1star:.1f}]")
    #---#
    ax.elev = 15
    ax.dist = -20
    ax.azim = 75
    ax.legend()
    ax.set_xlabel(r'$w_0$')  # x축 레이블 설정
    ax.set_ylabel(r'$w_1$')  # y축 레이블 설정
    ax.set_xticks([-10,-5,0])  # x축 틱 간격 설정
    ax.set_yticks([-10,0,10])  # y축 틱 간격 설정

def _learn_and_record(net, loss_fn, optimizr):
    yhat_history = []
    loss_history = []
    What_history = []
    Whatgrad_history = []
    What_history.append([net[0].bias.data.item(), net[0].weight.data.item()])
    for epoc in range(100):
        ## step1
        yhat = net(x)
        ## step2
        loss = loss_fn(yhat,y)
        ## step3
        loss.backward()
        ## step4
        optimizr.step()
        ## record
        if epoc % 5 ==0:
            yhat_history.append(yhat.reshape(-1).data.tolist())
            loss_history.append(loss.item())
            What_history.append([net[0].bias.data.item(), net[0].weight.data.item()])
            Whatgrad_history.append([net[0].bias.grad.item(), net[0].weight.grad.item()])
        optimizr.zero_grad()

    return yhat_history, loss_history, What_history, Whatgrad_history

def show_animation(net, loss_fn, optimizr):
    yhat_history,loss_history,What_history,Whatgrad_history = _learn_and_record(net,loss_fn,optimizr)

    fig = plt.figure(figsize=(7.5,3.5))
    ax1 = fig.add_subplot(1, 2, 1)
    ax2 = fig.add_subplot(1, 2, 2, projection='3d')
    ## ax1: 왼쪽그림
    ax1.scatter(x,y,alpha=0.01)
    ax1.scatter(x[0],y[0],color='C0',label=r"observed data = $(x_i,y_i)$")
    ax1.plot(x,prob,'--',label=r"prob (true) = $(x_i,\frac{exp(-1+5x_i)}{1+exp(-1+5x_i)})$")
    line, = ax1.plot(x,yhat_history[0],'--',label=r"prob (estimated) = $(x_i,\hat{y}_i)$")
    ax1.legend()
    ## ax2: 오른쪽그림
    plot_loss(loss_fn,ax2)
    ax2.scatter(np.array(What_history)[0,0],np.array(What_history)[0,1],loss_history[0],color='blue',s=200,marker='*')
    def animate(epoc):
        line.set_ydata(yhat_history[epoc])
        w0hat = np.array(What_history)[epoc,0]
        w1hat = np.array(What_history)[epoc,1]
        w0hatgrad = np.array(Whatgrad_history)[epoc,0]
        w1hatgrad = np.array(Whatgrad_history)[epoc,1]
        ax2.scatter(w0hat,w1hat,loss_history[epoc],color='grey')
        ax2.set_title(f"What.grad=[{w0hatgrad:.4f},{w1hatgrad:.4f}]",y=0.8)
        fig.suptitle(f"epoch={epoc*5} // What=[{w0hat:.2f},{w1hat:.2f}] // Loss={loss_fn.__class__.__name__} // Opt={optimizr.__class__.__name__}")
        return line
    ani = animation.FuncAnimation(fig, animate, frames=20)
    plt.close()
    return ani
```






























