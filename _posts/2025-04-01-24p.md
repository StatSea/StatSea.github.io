---
layout: single
title: "[CD] R을 활용한 웹 크롤링"
categories: [CD]
tags: [CD, Project, Crawling]
---

# R을 활용한 웹 크롤링 실습 요약

## 3.1 rvest를 활용한 기본 크롤링

### 학습 목표
- `rvest`로 네이버 뉴스 데이터 가져오기
- HTML 구조 분석 및 기사 제목/링크 크롤링
- 데이터 CSV로 저장하기

### 핵심 개념
- **웹 크롤링 vs 웹 스크래핑**
- `robots.txt` 확인 → 크롤링 제한 가능성 있음
- `read_html()`, `html_nodes()`, `html_text()`, `html_attr()` 등 사용

### 주요 코드 예시
```r
url <- "https://news.naver.com/main/ranking/popularDay.naver"
page <- read_html(url)

titles <- page %>% html_nodes(".rankingnews_box a") %>% html_text(trim = TRUE)
links <- page %>% html_nodes(".rankingnews_box a") %>% html_attr("href")
links <- ifelse(startsWith(links, "/"), paste0("https://news.naver.com", links), links)

news_data <- data.frame(제목 = titles, 링크 = links)
write.csv(news_data, "naver_news.csv", row.names = FALSE)
```

---

## 3.2 HTML 테이블 크롤링 및 저장

### 학습 목표
- HTML 테이블 데이터 크롤링
- `html_table()`로 데이터프레임 변환
- 데이터 정리 및 CSV 저장

### 주요 코드 예시
```r
url <- "https://ko.wikipedia.org/wiki/대한민국의_인구_변화"
tables <- read_html(url) %>% html_nodes("table") %>% html_table(fill = TRUE)

population_data <- tables[[2]]
colnames(population_data) <- c("연도", "총인구", "남성", "여성", "출생률", "사망률")
population_data <- population_data[-1, ] %>% na.omit()

write.csv(population_data, "population_data.csv", row.names = FALSE)
```

---

## 3.3 API 활용 및 JSON 처리

### 학습 목표
- `httr`로 API 요청
- `jsonlite`로 JSON 처리
- 데이터프레임 변환 후 CSV 저장

### 주요 코드 예시
```r
res <- GET("https://openapi.naver.com/v1/search/news.json?query=데이터+분석&display=5",
           add_headers("X-Naver-Client-Id" = "YOUR_CLIENT_ID",
                       "X-Naver-Client-Secret" = "YOUR_CLIENT_SECRET"))

json_data <- content(res, as = "text")
parsed_data <- fromJSON(json_data)

news_df <- parsed_data$items %>% select(title, link, description, pubDate)
write.csv(news_df, "naver_news_api.csv", row.names = FALSE)
```

---

## 3.4 페이지네이션 & 로그인 처리

### 학습 목표
- 페이지네이션 처리 (여러 페이지 반복 크롤링)
- `httr`로 로그인 후 세션 유지

### 페이지네이션 예시
```r
base_url <- "https://news.naver.com/main/ranking/popularDay.naver?page="
news_data <- data.frame()

for (page in 1:5) {
  url <- paste0(base_url, page)
  page_content <- read_html(url)
  titles <- page_content %>% html_nodes(".rankingnews_box a") %>% html_text(trim = TRUE)
  links <- page_content %>% html_nodes(".rankingnews_box a") %>% html_attr("href") %>%
           paste0("https://news.naver.com", .)
  news_data <- bind_rows(news_data, data.frame(제목 = titles, 링크 = links))
}
write.csv(news_data, "naver_news_pages.csv", row.names = FALSE)
```

---

## 3.5 자바스크립트 기반 크롤링 (RSelenium)

### 학습 목표
- 동적 페이지 크롤링
- `RSelenium`을 활용한 JS 요소 제어 (클릭, 스크롤 등)

### 🛠 기본 예시
```r
library(RSelenium)
rD <- rsDriver(browser = "chrome", chromever = "latest", verbose = FALSE)
remDr <- rD$client
remDr$navigate("https://news.naver.com")
print(remDr$getTitle())
remDr$close()
rD$server$stop()
```

---

## 3.6 실전 프로젝트 및 자동화

### 학습 목표
- 뉴스 트렌드 분석 프로젝트 구성
- `cronR`, `taskscheduleR`을 활용한 자동화 실행

### 🛠 실전 예시
```r
page <- read_html("https://news.naver.com/main/ranking/popularDay.naver")
titles <- page %>% html_nodes(".rankingnews_box a") %>% html_text(trim = TRUE)
links <- page %>% html_nodes(".rankingnews_box a") %>% html_attr("href") %>% paste0("https://news.naver.com", .)

news_data <- data.frame(날짜 = Sys.Date(), 제목 = titles, 링크 = links)
if (file.exists("naver_news_trend.csv")) {
  old_data <- read.csv("naver_news_trend.csv")
  news_data <- bind_rows(old_data, news_data)
}
write.csv(news_data, "naver_news_trend.csv", row.names = FALSE)
```

### 자동화 예시 (Linux)
```r
library(cronR)
cmd <- cron_rscript("/home/statcomp/naver_news_trend.R")
cron_add(cmd, frequency = "daily", at = "09:00", id = "naver_news")
```

### 자동화 예시 (Windows)
```r
library(taskscheduleR)
taskscheduler_create(taskname = "naver_news_trend",
                     rscript = "C:/path/to/naver_news_trend.R",
                     schedule = "DAILY", starttime = "09:00")
```

---
