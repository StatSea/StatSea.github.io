---
layout: single
title: "[DL] 딥러닝 02wk"
categories: [DL]
tags: [DL]
mathjax: true
---
경사하강법을 이용하여 더 나은 직선 찾기

## 기본 세팅

``` python
import torch
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (4.5, 3.0)
```

## 미분값 계산하기
- loss 업데이트는 원래What - 0.1미분값 으로 하면 된다.

### 계산법 1

1. loss를 구하는 함수를 만든다.
2. 
``` python
def l(w0,w1):
    yhat = w0 + w1*x
    return torch.sum((y-yhat)**2)
```

2. h = 0.001로 설정하여 미분 식에 대입하기

```python
h = 0.001
a = torch.tensor([[(l(-5+h,10) - l(-5,10))/h ], [(l(-5,10+h) - l(-5,10))/h]])
```
3. 업데이트 하기

```python
s = What - 0.001 * a
```

4. 산점도 그리기

```python
plt.plot(x,y,'o')
plt.plot(x,X@What,'-') # 원래What: 주황색
plt.plot(x,X@s,'-') # 더나은What: 초록색
```

- 이 방법은 잘 된거 같긴하지만 미분 구하는게 어렵다.

### 계산법 2

$$\frac{\partial}{\partial {\bf W}}loss({\bf W})= -2{\bf X}^\top {\bf y} + 2{\bf X}^\top {\bf X}{\bf W}$$

```python
-2*X.T@y + 2*X.T@X@What
```

### 계산법 3

```python
What = torch.tensor([[-5.0],[10.0]],requires_grad=True)
yhat = X@What
loss = torch.sum((y-yhat)**2)
loss.backward()
What.grad
```
- backward()로 미분
- grad는 미분값을 출력해줌

#### 차근차근 계산해 보기

```python
Wbefore = What.data
Wafter = What.data - alpha * What.grad
Wbefore, Wafter
```
- 산점도를 그려본다면?

![image](https://github.com/user-attachments/assets/64bf1325-e005-42bb-836a-e961dd40bcab)


## 3단계 -- iteration , w의 추정치

- 1,2단계 코드를 먼저 쓴다.

```python
# 최초의 직선을 만드는 값
What = torch.tensor([[-5.0],[10.0]],requires_grad=True)
for epoc in range(30):
    yhat = X@What # 행렬곱
    loss = torch.sum((y-yhat)**2) # loss 식 적용
    loss.backward() # 미분
    What.data = What.data - 0.001 * What.grad # 미분값 적용
    What.grad = None # 미분 값 초기화
```
- 초기화를 해주는 이유 : `What.grad` $\leftarrow$ `What.grad` + `What에서미분값`이기 때문이다.













