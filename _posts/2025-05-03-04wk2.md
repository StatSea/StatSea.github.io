---
layout: single
title: "[DL] 딥러닝 04wk2"
categories: [DL]
tags: [DL]
mathjax: true
---

딥러닝 04wk2 정리

---

# 기본 세팅
```python
import torch
import torchvision
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

plt.rcParams['figure.figsize'] = (4.5, 3.0)
```

# 1. 꺾인 그래프의 한계?

```python
## 지난시간
# 목표: 꺽여있는 곡선을 만드는것
# sig(꺽여있는직선)
#---#
# net = torch.nn.Sequential(
#     torch.nn.Linear(1,2,bias=False), ## 두개의 직선 (기울기와 절편이 각각다른)
#     torch.nn.ReLU(), # 두개의 ReLU직선 (relu: 양수는 그대로, 음수는 0으로)
#     torch.nn.Linear(2,1), # 하나의 꺾인직선 (두개를 하나로 합쳐서..)
#     torch.nn.Sigmoid(), # 하나의 꺾이는곡선
# )
# (n,1) --l1--> (n,2) --a1--> (n,2) ---l2---> (n,1) ---a2---> (n,1)
```

- 이 기술은 sig를 취하기 전이 꺾은 선일 때만 가능해서 표현력이 부족할 것 같다.
- 하지만 생각보다 표현력이 풍부하다.

## A. step은 표현이 불가능하지 않나?

`-` 예제1 : 취업 합격률 곡선

```python
torch.manual_seed(43052)
x = torch.linspace(-1,1,2000).reshape(-1,1)
u = 0*x-3
u[x<-0.2] = (15*x+6)[x<-0.2]
u[(-0.2<x)&(x<0.4)] = (0*x-1)[(-0.2<x)&(x<0.4)]
sig = torch.nn.Sigmoid()
v = π = sig(u)
y = torch.bernoulli(v)

plt.plot(x,y,'.',alpha=0.03, label="observed")
plt.plot(x,v,'--', label="unobserved")
plt.legend()
```
 ![image](https://github.com/user-attachments/assets/234c8c32-473d-471e-ab4d-540bda1e0ec4)

```python
net = torch.nn.Sequential(
    torch.nn.Linear(1,512),
    torch.nn.ReLU(),
    torch.nn.Linear(512,1),
    torch.nn.Sigmoid()
)
loss_fn = torch.nn.BCELoss()
optimizr = torch.optim.Adam(net.parameters())
#---#
for epoc in range(5000):
    ## 1
    yhat = net(x)
    ## 2
    loss = loss_fn(yhat,y)
    ## 3
    loss.backward()
    ## 4
    optimizr.step()
    optimizr.zero_grad()


plt.plot(x,y,'.',alpha=0.03, label="observed")
plt.plot(x,v, label="true")
plt.plot(x,net(x).data,'--', label="estimated")
plt.legend()
```

- 은닉 노드수가 512개
![image](https://github.com/user-attachments/assets/4079c16c-1496-4237-bb23-9b0385971b47)


## B. 곡선은 표현 불가능하지 않나?

`-` 예제 2 : 2024년 수능 미적30번 문제에 나온 곡선

$$y_i = e^{-x_i} \times  |\cos(5x_i)| \times \sin(5x) + \epsilon_i, \quad \epsilon_i \sim N(0,\sigma^2)$$

```python
torch.manual_seed(43052)
x = torch.linspace(0,2,2000).reshape(-1,1)
eps = torch.randn(2000).reshape(-1,1)*0.05
fx = torch.exp(-1*x)* torch.abs(torch.cos(3*x))*(torch.sin(3*x))
y = fx + eps

plt.plot(x,y,label="observed",alpha=0.5)
plt.plot(x,fx,label="true")
```
![image](https://github.com/user-attachments/assets/045fb2aa-2e50-4a10-b222-a58eae37a3a8)

```python
net = torch.nn.Sequential(
    torch.nn.Linear(1,2048), # 꺽이지않은 1024개의 직선
    torch.nn.ReLU(), # 꺽인(렐루된) 1024개의 직선
    torch.nn.Linear(2048,1), # 합쳐진 하나의 꺽인 직선
)
loss_fn = torch.nn.MSELoss()
optimizr = torch.optim.Adam(net.parameters())
##
for epoc in range(1000):
    ## 1
    yhat = net(x)
    ## 2
    loss = loss_fn(yhat,y)
    ## 3
    loss.backward()
    ## 4
    optimizr.step()
    optimizr.zero_grad()

plt.plot(x,y,label="observed",alpha=0.5)
plt.plot(x,fx,label="true")
plt.plot(x,net(x).data,'--',label="estimated")
plt.legend()
```
![image](https://github.com/user-attachments/assets/0aa013bf-9be0-4a4f-98dc-cbc67406de5c)


# 2. 시벤코 정리

## A. 시벤코 정리 소개
- 예시 1 : 어떤 복잡한 함수를 모를때 보편 근사 정리는 신경망이 복잡한 함수를 거의 똑같이 흉내낼 수 있다고 함
    - 조건 : 은닉층 하나, 은닉 노드 수 충분히 많게, 활성화 함수는 sigmoid같은 연속적 함수

```python
net = torch.nn.Sequential(
    torch.nn.Linear(p, ???),      # 입력 p차원 → 은닉층
    torch.nn.Sigmoid(),           # 비선형성
    torch.nn.Linear(???, q)       # 은닉층 → 출력 q차원
)
```

- 보렐 가측 함수 : 수학적으로 다룰 수 있는 대부분의 함수
  - 예시 : y = x^2, y = sin(x) , 토익점수 → 합격 확률, 이미지 → 고양이냐 아니냐

| 질문                      | 답변                                        |
| ----------------------- | ----------------------------------------- |
| 신경망이 정말 아무 함수나 배울 수 있어? | ✅ 네, 이론적으로 가능해요!                          |
| 층이 하나만 있어도 돼?           | ✅ 네, 은닉층 하나면 충분해요. (노드는 많아야 함)            |
| 어떤 종류의 함수까지 가능한데?       | ✅ 우리가 실생활에서 쓰는 함수 대부분 다 가능해요.             |
| 조건은 뭐야?                 | ✔ 은닉층 하나<br>✔ 충분한 노드 수<br>✔ 비선형 활성화 함수 사용 |

- 결론 : 하나의 은닉층만 있는 신경망도 충분한 노드만 있다면 세상에 존재하는 거의 모든 규칙이나 패턴을 근사할 수 있다.


## B. 왜 이게 가능할까?

- 준비
```python
x = torch.linspace(-10,10,200).reshape(-1,1)
net = torch.nn.Sequential(
    torch.nn.Linear(in_features=1,out_features=2),
    torch.nn.Sigmoid(),
    torch.nn.Linear(in_features=2,out_features=1)
)
l1,a1,l2 = net

net
```
![image](https://github.com/user-attachments/assets/0efc5b1a-2bea-41a9-ae6a-96f12a5e50d1)

### 생각 1 - 2개의 시그모이드를 우연히 잘 조합하면 하나의 계단함수를 만들 수 있다.
```python
l1.weight.data = torch.tensor([[-5.00],[5.00]])
l1.bias.data = torch.tensor([+10.00,+10.00])

l2.weight.data = torch.tensor([[1.00,1.00]])
l2.bias.data = torch.tensor([-1.00])

fig,ax = plt.subplots(1,3,figsize=(9,3))
ax[0].plot(x,l1(x)[:,[0]].data,label=r"$-5x+10$")
ax[0].plot(x,l1(x)[:,[1]].data,label=r"$5x+10$")
ax[0].set_title('$l_1(x)$')
ax[0].legend()
ax[1].plot(x,a1(l1(x))[:,[0]].data,label=r"$v_1=sig(-5x+10)$")
ax[1].plot(x,a1(l1(x))[:,[1]].data,label=r"$v_2=sig(5x+10)$")
ax[1].set_title('$(a_1 \circ l_1)(x)$')
ax[1].legend()
ax[2].plot(x,l2(a1(l1(x))).data,color='C2',label=r"$v_1+v_2-1$")
ax[2].set_title('$(l_2 \circ a_1 \circ \l_1)(x)$')
ax[2].legend()
```
![image](https://github.com/user-attachments/assets/6b41b1f6-8c25-4727-9437-8ef844ccddf6)

### 생각 2 - 계단함수의 모양이 생각 1과 같을 필요가 없다. 중심은 이동 가능하고 높이도 조절 가능하다.

```python
l1.weight.data = torch.tensor([[-5.00],[5.00]])
l1.bias.data = torch.tensor([+0.00,+20.00])
l2.weight.data = torch.tensor([[1.00,1.00]])
l2.bias.data = torch.tensor([-1.00])
fig,ax = plt.subplots(1,3,figsize=(9,3))
ax[0].plot(x,l1(x).data.numpy(),'--',color='C0'); ax[0].set_title('$l_1(x)$')
ax[1].plot(x,a1(l1(x)).data.numpy(),'--',color='C0'); ax[1].set_title('$(a_1 \circ l_1)(x)$')
ax[2].plot(x,l2(a1(l1(x))).data,'--',color='C0'); ax[2].set_title('$(l_2 \circ a_1 \circ \l_1)(x)$');
ax[2].set_ylim(-0.1,2.6)
```
![image](https://github.com/user-attachments/assets/9840fb6f-b084-4eed-a71a-ed0e35e59e4d)


```python
l1.weight.data = torch.tensor([[-5.00],[5.00]])
l1.bias.data = torch.tensor([+20.00,+00.00])
l2.weight.data = torch.tensor([[2.50,2.50]])
l2.bias.data = torch.tensor([-2.50])
fig,ax = plt.subplots(1,3,figsize=(9,3))
ax[0].plot(x,l1(x).data.numpy(),'--',color='C1'); ax[0].set_title('$l_1(x)$')
ax[1].plot(x,a1(l1(x)).data.numpy(),'--',color='C1'); ax[1].set_title('$(a_1 \circ l_1)(x)$')
ax[2].plot(x,l2(a1(l1(x))).data,'--',color='C1'); ax[2].set_title('$(l_2 \circ a_1 \circ \l_1)(x)$');
ax[2].set_ylim(-0.1,2.6)
```
![image](https://github.com/user-attachments/assets/d1c8ec91-540a-4b54-bf83-e25d06ff3be3)


### 생각 3 - 첫번째 선형변환(=$l_1$)에서 `out_features=4`로 하고 적당한 가중치를 조정하면 $(l_2\circ a_1 \circ l_1)(x)$의 결과로 생각2의 예시1,2를 조합한 형태도 가능할 것 같다. 즉 4개의 시그모이드를 잘 조합하면 2단계 계단함수를 만들 수 있다.
```python
l1 = torch.nn.Linear(in_features=1,out_features=4)
a1 = torch.nn.Sigmoid()
l2 = torch.nn.Linear(in_features=4,out_features=1)

l1.weight.data = torch.tensor([[-5.00],[5.00],[-5.00],[5.00]])
l1.bias.data = torch.tensor([0.00, 20.00, 20.00, 0])
l2.weight.data = torch.tensor([[1.00,  1.00, 2.50,  2.50]])
l2.bias.data = torch.tensor([-1.0-2.5])

plt.plot(l2(a1(l1(x))).data,'--')
plt.title(r"$(l_2 \circ a_1 \circ l_1)(x)$")
```
![image](https://github.com/user-attachments/assets/c1cfd8fe-5d66-43b2-9282-ddc43fc62713)

> 이러한 함수는 계단 모양이며, 0을 제외한 서로 다른 계단의 높이는 2개가 된다. 이를 간단히 2단계 - 계단함수라고 칭하자.


### 생각 4 - $2m$개의 시그모이드를 우연히 잘 조합하면 $m$단계 계단함수를 만들 수 있다.

`-` 정리1: 2개의 시그모이드를 우연히 잘 결합하면 아래와 같은 "1단계-계단함수" 함수 $h$를 만들 수 있다.

```python
def h(x):
    sig = torch.nn.Sigmoid()
    v1 = -sig(200*(x-0.5))
    v2 = sig(200*(x+0.5))
    return v1+v2

plt.plot(x,h(x))
plt.title("$h(x)$")
```
![image](https://github.com/user-attachments/assets/0317c86f-3263-46c4-98c5-ae00879da406)

![image](https://github.com/user-attachments/assets/6bb6e7de-3c66-435d-9dd3-08a6366aa1cf)

### 생각 5 - 어지간한 함수형태는 구불구불한 m단계 - 계단함수로 다 근사할 수 있지 않을까?

- 아래의 네트워크에서 1. ?? 을 충분히 키우고 2. 적절하게 학습만 잘 된다면 거의 무한한 표현력을 가질 수 있음

```python
net = torch.nn.Sequential(
    torch.nn.Linear(p,???),
    torch.nn.Sigmoid(),
    torch.nn.Linear(???,q)
)
```

## C. h의 위력

- 아래처럼 net을 설계해서 위력을 체감해보고 싶은 경우

```python
net = torch.nn.Sequential(
    torch.nn.Linear(1,??),
    torch.nn.H(),
    torch.nn.Linear(??,1)
)
```

- h(x)를 생성하는 클래스를 만들어 보기

```python
class H(torch.nn.Module):
    def __init__(self):
        super().__init__()
    def forward(self,x):
        def h(x):
            sig = torch.nn.Sigmoid()
            v1 = -sig(200*(x-0.5))
            v2 = sig(200*(x+0.5))
            return v1+v2
        out = h(x)
        return out

h = H()
```

- 이제 h의 위력을 체감해보자.

### 예제 1 - 스펙의 역설

```python
df = pd.read_csv("https://raw.githubusercontent.com/guebin/DL2025/main/posts/ironyofspec.csv")
x = torch.tensor(df.x).float().reshape(-1,1)
y = torch.tensor(df.y).float().reshape(-1,1)
prob = torch.tensor(df.prob).float().reshape(-1,1)

net = torch.nn.Sequential(
    torch.nn.Linear(1,2048),
    H(),
    torch.nn.Linear(2048,1),
    torch.nn.Sigmoid()
)
loss_fn = torch.nn.BCELoss()
optimizr = torch.optim.Adam(net.parameters())
#---#
for epoc in range(200):
    ## 1
    yhat = net(x)
    ## 2
    loss = loss_fn(yhat,y)
    ## 3
    loss.backward()
    ## 4
    optimizr.step()
    optimizr.zero_grad()

plt.plot(x,prob)
plt.plot(x,net(x).data,'--')
```

![image](https://github.com/user-attachments/assets/d565d7f7-9ce5-46c4-824c-0a113c76fc95)



### 예제 2 - 수능곡선
```python
torch.manual_seed(43052)
x = torch.linspace(0,2,2000).reshape(-1,1)
eps = torch.randn(2000).reshape(-1,1)*0.05
fx = torch.exp(-1*x)* torch.abs(torch.cos(3*x))*(torch.sin(3*x))
y = fx + eps

plt.plot(x,y,alpha=0.5)
plt.plot(x,fx)
```
![image](https://github.com/user-attachments/assets/a2dcc035-1f3d-4424-89c3-d20ba097940b)

```python
net = torch.nn.Sequential(
    torch.nn.Linear(1,2048),
    H(),
    torch.nn.Linear(2048,1)
)
loss_fn = torch.nn.MSELoss()
optimizr = torch.optim.Adam(net.parameters())
#---#
for epoc in range(200):
    ## 1
    yhat = net(x)
    ## 2
    loss = loss_fn(yhat,y)
    ## 3
    loss.backward()
    ## 4
    optimizr.step()
    optimizr.zero_grad()

plt.plot(x,y,alpha=0.5)
plt.plot(x,fx)
plt.plot(x,net(x).data,'--')
```


![image](https://github.com/user-attachments/assets/251b6534-0ad7-427c-ad38-89930ddb8ad0)

## D. 의문점
- 그냥 활성화 함수를 h로 쓰면 끝 아닌가? 뭐하러 relu를 쓰지?
- 딥러닝을 좀 공부해본사람1: 왜 딥러닝이 2010년이 지나서야 떳지? 1989년에 세상의 모든 문제가 풀려야 하는것 아닌가?
- 딥러닝을 좀 공부해본사람2: 하나의 은닉층을 가진 네크워크는 잘 안쓰지 않나? 은닉층이 깊을수록 좋다고 들었는데?
- 약간의 의구심이 있지만 아무튼 우리는 아래의 무기를 가진 꼴이 되었다.


```python
net = torch.nn.Sequential(
    torch.nn.Linear(p,???),
    torch.nn.Sigmoid(),
    torch.nn.Linear(???,q)
)

```

- 하나의 은닉층을 가지는 네트워크로 모든 보렐 가측 함수를 원하는 정확도로 근사시킬 수 있음

# 3. MNIST 해결

## A. 예비학습 -- plt.imshow()

`-` `plt.imshow(..., cmap="gray")` 에서 `...`이 shape이 (??,??)이면 흑백이미지를 출력

```python
img = torch.tensor([[255,100],
                    [255,0]])
plt.imshow(img,cmap="gray")
```
![image](https://github.com/user-attachments/assets/ba4f9796-4800-4cbb-9aa5-ed728db6564e)


`-` `plt.imshow(...)` 에서 `...`의 shape이 (??,??,3)이면 칼라이미지를 출력

```python
r = torch.tensor([[255,0],
                  [255,0]])
g = torch.tensor([[0,255],
                  [0,0]])
b = torch.tensor([[0,0],
                  [0,255]])
img = torch.stack([r,g,b],axis=-1)
plt.imshow(img)
```
![image](https://github.com/user-attachments/assets/1d960caa-a408-4b36-aa3c-b15bf3e5dcc9)

`-` `plt.imshow(...)` 에서 `...`의 자료형이 int인지 float인지에 따라서 인식이 다름

```python
r = torch.tensor([[1,0],
                  [1,0]])
g = torch.tensor([[0,1],
                  [0,0]])
b = torch.tensor([[0,0],
                  [0,1]])
img = torch.stack([r,g,b],axis=-1)
plt.imshow(img)

```
![image](https://github.com/user-attachments/assets/f6bb7fbc-e48c-4c04-9e0a-414aa07dd029)

```python
r = torch.tensor([[255,0],
                  [255,0]])/255
g = torch.tensor([[0,255],
                  [0,0]])/255
b = torch.tensor([[0,0],
                  [0,255]])/255
img = torch.stack([r,g,b],axis=-1)
plt.imshow(img)
```
![image](https://github.com/user-attachments/assets/5f9291e4-7c30-46a8-bc72-b51d42f7316e)

## B. 데이터

```python
train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)
to_tensor = torchvision.transforms.ToTensor()
X3 = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==3])
X7 = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==7])
X = torch.concat([X3,X7],axis=0)
y = torch.tensor([0.0]*len(X3) + [1.0]*len(X7))

plt.plot(y,'.')
```
![image](https://github.com/user-attachments/assets/9f61e7b8-0965-4b43-a01a-192465081389)

`-` 우리는 ${\bf X}: (n,1,28,28)$ 에서 ${\bf y}: (n,1)$으로 가는 맵핑을 배우고 싶음. $\to$ 이런건 배운적이 없는데?.. $\to$ 그렇다면 ${\bf X}:(n,784) \to {\bf y}:(n,1)$ 으로 가는 맵핑을 학습하자.

```python
X = torch.stack([img.reshape(-1) for img in X])
y = y.reshape(-1,1)

X.shape,y.shape
```
![image](https://github.com/user-attachments/assets/295fc2c8-3412-4631-be1c-5a07575418f1)

## C. 학습

```python
net = torch.nn.Sequential(
    torch.nn.Linear(784,32),
    torch.nn.ReLU(),
    torch.nn.Linear(32,1),
    torch.nn.Sigmoid()
)
loss_fn = torch.nn.BCELoss()
optimizr = torch.optim.Adam(net.parameters())
#---#
for epoc in range(200):
    ## 1
    yhat = net(X)
    ## 2
    loss = loss_fn(yhat,y)
    ## 3
    loss.backward()
    ## 4
    optimizr.step()
    optimizr.zero_grad()


plt.plot(y,'.')
plt.plot(net(X).data,'.',alpha=0.2)
```
![image](https://github.com/user-attachments/assets/4e215934-1667-428b-894e-944a43502a46)


- 신경망의 예측 정확도 계산
```python
((y == (net(X).data > 0.5))*1.0).mean()
```


**이미지자료의 차원**

- 칼라이미지데이터 ${\bf X}$는 (n,3,h,w) 의 차원을 가지거나 (n,h,w,3)의 차원을 가진다.
- 흑백이미지데이터 ${\bf X}$는 (n,h,w) 의 차원을 가지거나 (n,1,h,w)의 차원을 가지거나 (n,h,w,1)의 차원을 가진다.
